{"cells":[{"cell_type":"markdown","id":"prostate-arizona","metadata":{"id":"bA5ajAmk7XH6"},"source":["# Introduction to Deep Learning in Python - Datacamp, Dan Becker\n","Notes by CÃ©sar.  \n"]},{"cell_type":"code","execution_count":1,"id":"2e25fdd8-4d84-45bc-80f0-949917e00a17","metadata":{"collapsed":false,"executionTime":170,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastSuccessfullyExecutedCode":"# Import pandas\nimport pandas as pd\n\n# Import the course datasets \nwages = pd.read_csv('datasets/hourly_wages.csv')\nmnist = pd.read_csv('datasets/mnist.csv')\ntitanic = pd.read_csv('datasets/titanic_all_numeric.csv')"},"outputs":[],"source":["# Import pandas\n","import pandas as pd\n","\n","# Import the course datasets \n","url_1=\"https://raw.githubusercontent.com/cmuro27/Machine_Learning_Projects_and_Notes/main/datasets/hourly_wages.csv\"\n","url_2=\"https://raw.githubusercontent.com/cmuro27/Machine_Learning_Projects_and_Notes/main/datasets/titanic_all_numeric.csv\"\n","wages = pd.read_csv(url_1)\n","titanic = pd.read_csv(url_2)"]},{"cell_type":"markdown","id":"0e7949e8","metadata":{},"source":["## Basics of neural networks and deep learning  \n","\n","Neural networks accounts for interactions.  \n","We have something called an *input layer*. This represents our predictive features. On the far right we have the *output layer*. The prediction from our model. All layers that are not the input or output layers are named *hidden layers*. More nodes or neurons in the hidden layers, then more interactions.   \n","  \n","They are called hidden layers because, while the inputs and outputs correspond to visible things that happened in the world, and they can be stored as data, the values in the hidden layer aren't something we have data about, or anything we observe directly from the world. Nevertheless, each dot, called a node, in the hidden layer, represents an aggregation of information "]},{"cell_type":"markdown","id":"456a6590-a580-46a9-8199-82824ea581f2","metadata":{},"source":["**Forward propagation**   \n","How neural networks makes predicitons is called forward propagation algorithm.  \n","* Multiply ad process at the jth layer with the ith neuron: $\\vec{w}_{j}^{[i]}\\cdot \\vec{x}^{i-1}+b^{[i]}_{j}$\n","* Forward propagation for one data point at time\n","* Output is the prediciton for the data point"]},{"cell_type":"markdown","id":"b44ac676-3107-41ff-ad87-2c48631f24e6","metadata":{},"source":["**Coding the forward propagation algorithm**"]},{"cell_type":"code","execution_count":2,"id":"242cae23-c840-436f-aa68-7b8a15ff59b5","metadata":{"executionTime":92,"lastSuccessfullyExecutedCode":"import numpy as np\nweights={'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\ninput_data=np.array([3, 5])\n# Calculate node 0 value: node_0_value\nnode_0_value = (input_data * weights['node_0']).sum()\n\n# Calculate node 1 value: node_1_value\nnode_1_value = (input_data*weights['node_1']).sum()\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_value, node_1_value])\n\n# Calculate output: output\noutput = (hidden_layer_outputs*weights['output']).sum()\n\n# Print output\nprint(output)"},"outputs":[{"name":"stdout","output_type":"stream","text":["-39\n"]}],"source":["import numpy as np\n","weights={'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n","input_data=np.array([3, 5])\n","# Calculate node 0 value: node_0_value\n","node_0_value = (input_data * weights['node_0']).sum()\n","\n","# Calculate node 1 value: node_1_value\n","node_1_value = (input_data*weights['node_1']).sum()\n","\n","# Put node values into array: hidden_layer_outputs\n","hidden_layer_outputs = np.array([node_0_value, node_1_value])\n","\n","# Calculate output: output\n","output = (hidden_layer_outputs*weights['output']).sum()\n","\n","# Print output\n","print(output)"]},{"cell_type":"markdown","id":"1ad9762f-160d-49fa-9888-f33fdd225d52","metadata":{},"source":["It looks like the network generated a prediction of -39."]},{"cell_type":"markdown","id":"2fbcbec7-20c4-4896-8e04-1d6058760208","metadata":{},"source":["**Activition functions**\n","For neural networks to achieve their maximum predictive power, we must apply something called an activation function in the hidden layers. An activation function allows the model to capture non-linearities.\n","* Applied to node inputs to produce node output\n","* Usually, the activation function is th sigmoid function, which is a simple hyperbolic tangent  "]},{"cell_type":"markdown","id":"a1b20cfa-3607-4ff2-b048-f9314e3ccc46","metadata":{},"source":["An \"activation function\" is a function applied at each node. It converts the node's input into some output.\n","\n","The rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks.\n","$$f(x)=max(x,0)$$\n","This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive. "]},{"cell_type":"code","execution_count":7,"id":"fa3ac299-6b38-4985-a2c2-2ac032253b63","metadata":{"executionTime":49,"lastSuccessfullyExecutedCode":"def relu(input):\n    '''Define your relu activation function here'''\n    # Calculate the value for the output of the relu function: output\n    output = max(input, 0)\n    \n    # Return the value just calculated\n    return(output)\n\n# Calculate node 0 value: node_0_output\nnode_0_input = (input_data * weights['node_0']).sum()\nnode_0_output = relu(node_0_input)\n\n# Calculate node 1 value: node_1_output\nnode_1_input = (input_data * weights['node_1']).sum()\nnode_1_output = relu(node_1_input)\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_output, node_1_output])\n\n# Calculate model output (do not apply relu)\nmodel_output = (hidden_layer_outputs * weights['output']).sum()\n\n# Print model output\nprint(model_output)"},"outputs":[{"name":"stdout","output_type":"stream","text":["52\n"]}],"source":["def relu(input):\n","    '''Define your relu activation function here'''\n","    # Calculate the value for the output of the relu function: output\n","    output = max(input, 0)\n","    \n","    # Return the value just calculated\n","    return(output)\n","\n","# Calculate node 0 value: node_0_output\n","node_0_input = (input_data * weights['node_0']).sum()\n","node_0_output = relu(node_0_input)\n","\n","# Calculate node 1 value: node_1_output\n","node_1_input = (input_data * weights['node_1']).sum()\n","node_1_output = relu(node_1_input)\n","\n","# Put node values into array: hidden_layer_outputs\n","hidden_layer_outputs = np.array([node_0_output, node_1_output])\n","\n","# Calculate model output (do not apply relu)\n","model_output = (hidden_layer_outputs * weights['output']).sum()\n","\n","# Print model output\n","print(model_output)"]},{"cell_type":"markdown","id":"ee705efd-5f4e-46fd-880c-620337f0ba9f","metadata":{},"source":["You predicted 52 transactions. Without this activation function, you would have predicted a negative number! The real power of activation functions will come soon when you start tuning model weights."]},{"cell_type":"markdown","id":"ed9ab7d8-5a15-4b0d-8b6c-5c32e783b5bd","metadata":{},"source":["You'll now define a function called predict_with_network() which will generate predictions for multiple data observations, which are pre-loaded as input_data. As before, weights are also pre-loaded. In addition, the relu() function you defined in the previous exercise has been pre-loaded."]},{"cell_type":"code","execution_count":9,"id":"d1aabf32-59fd-4c25-83ab-c14dde57f128","metadata":{"executionTime":45,"lastSuccessfullyExecutedCode":"import numpy as np\ninput_data=[np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\nweights={'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n\n# Define predict_with_network()\ndef predict_with_network(input_data_row, weights):\n\n    # Calculate node 0 value\n    node_0_input = (input_data_row*weights['node_0']).sum()\n    node_0_output = relu(node_0_input)\n\n    # Calculate node 1 value\n    node_1_input = (node_0_output*weights['node_1']).sum()\n    node_1_output = relu(node_1_input)\n\n    # Put node values into array: hidden_layer_outputs\n    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n    \n    # Calculate model output\n    input_to_final_layer = (hidden_layer_outputs*weights['output']).sum()\n    model_output = relu(input_to_final_layer)\n    \n    # Return model output\n    return(model_output)\n\n# Create empty list to store prediction results\nresults = []\nfor input_data_row in input_data:\n    # Append prediction to results\n    results.append(predict_with_network(input_data_row,weights))\n\n# Print results\nprint(results)     "},"outputs":[{"name":"stdout","output_type":"stream","text":["[52, 0, 0, 64]\n"]}],"source":["import numpy as np\n","input_data=[np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n","weights={'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n","\n","# Define predict_with_network()\n","def predict_with_network(input_data_row, weights):\n","\n","    # Calculate node 0 value\n","    node_0_input = (input_data_row*weights['node_0']).sum()\n","    node_0_output = relu(node_0_input)\n","\n","    # Calculate node 1 value\n","    node_1_input = (node_0_output*weights['node_1']).sum()\n","    node_1_output = relu(node_1_input)\n","\n","    # Put node values into array: hidden_layer_outputs\n","    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n","    \n","    # Calculate model output\n","    input_to_final_layer = (hidden_layer_outputs*weights['output']).sum()\n","    model_output = relu(input_to_final_layer)\n","    \n","    # Return model output\n","    return(model_output)\n","\n","# Create empty list to store prediction results\n","results = []\n","for input_data_row in input_data:\n","    # Append prediction to results\n","    results.append(predict_with_network(input_data_row,weights))\n","\n","# Print results\n","print(results)     "]},{"cell_type":"markdown","id":"d0975bf7-74c5-4902-a1ed-34310bbb9f5d","metadata":{},"source":["**Deeper networks**  \n"," In practice, it's becoming common to have neural networks that have many, many layers; five layers, ten layers. A few years ago 15 layers was state of the art but this can scale quite naturally to even a thousand layers. \n","* Deep networks internally build represantations of patterns in the data \n","* Partially, replace the need for feature engineering\n","* Deep learning is also sometimes called representation learning, because subsequent layers build\n","* Subsequente layers build increasingly sophiscated representations of raw data\n","* Modeler does not need to specify the interactions \n","* Neural networks find relevant patterns to make better predictions \n","* The last layers capture the most complex interactions "]},{"cell_type":"code","execution_count":2,"id":"055d6960-eb4b-4036-ba99-218ba6953a56","metadata":{"executionTime":116,"lastSuccessfullyExecutedCode":"import numpy as np\n\ndef relu(input):\n    '''Define your relu activation function here'''\n    # Calculate the value for the output of the relu function: output\n    output = max(input, 0)\n    \n    # Return the value just calculated\n    return(output)\n\ninput_data=np.array([3, 5])  \nweights={'node_0_0': np.array([2, 4]),\n 'node_0_1': np.array([ 4, -5]),\n 'node_1_0': np.array([-1,  2]),\n 'node_1_1': np.array([1, 2]),\n 'output': np.array([2, 7])}\ndef predict_with_network(input_data):\n    # Calculate node 0 in the first hidden layer\n    node_0_0_input = (input_data * weights['node_0_0']).sum()\n    node_0_0_output = relu(node_0_0_input)\n\n    # Calculate node 1 in the first hidden layer\n    node_0_1_input = (input_data*weights['node_0_1']).sum()\n    node_0_1_output = relu(node_0_1_input)\n\n    # Put node values into array: hidden_0_outputs\n    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n    \n    # Calculate node 0 in the second hidden layer\n    node_1_0_input = (hidden_0_outputs*weights['node_1_0']).sum()\n    node_1_0_output = relu(node_1_0_input)\n\n    # Calculate node 1 in the second hidden layer\n    node_1_1_input = (hidden_0_outputs*weights['node_1_1']).sum()\n    node_1_1_output = relu(node_1_1_input)\n\n    # Put node values into array: hidden_1_outputs\n    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n\n    # Calculate model output: model_output\n    model_output = (hidden_1_outputs*weights['output']).sum()\n    \n    # Return model_output\n    return(model_output)\n\noutput = predict_with_network(input_data)\nprint(output)"},"outputs":[{"name":"stdout","output_type":"stream","text":["182\n"]}],"source":["import numpy as np\n","\n","def relu(input):\n","    '''Define your relu activation function here'''\n","    # Calculate the value for the output of the relu function: output\n","    output = max(input, 0)\n","    \n","    # Return the value just calculated\n","    return(output)\n","\n","input_data=np.array([3, 5])  \n","weights={'node_0_0': np.array([2, 4]),\n"," 'node_0_1': np.array([ 4, -5]),\n"," 'node_1_0': np.array([-1,  2]),\n"," 'node_1_1': np.array([1, 2]),\n"," 'output': np.array([2, 7])}\n","def predict_with_network(input_data):\n","    # Calculate node 0 in the first hidden layer\n","    node_0_0_input = (input_data * weights['node_0_0']).sum()\n","    node_0_0_output = relu(node_0_0_input)\n","\n","    # Calculate node 1 in the first hidden layer\n","    node_0_1_input = (input_data*weights['node_0_1']).sum()\n","    node_0_1_output = relu(node_0_1_input)\n","\n","    # Put node values into array: hidden_0_outputs\n","    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n","    \n","    # Calculate node 0 in the second hidden layer\n","    node_1_0_input = (hidden_0_outputs*weights['node_1_0']).sum()\n","    node_1_0_output = relu(node_1_0_input)\n","\n","    # Calculate node 1 in the second hidden layer\n","    node_1_1_input = (hidden_0_outputs*weights['node_1_1']).sum()\n","    node_1_1_output = relu(node_1_1_input)\n","\n","    # Put node values into array: hidden_1_outputs\n","    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n","\n","    # Calculate model output: model_output\n","    model_output = (hidden_1_outputs*weights['output']).sum()\n","    \n","    # Return model_output\n","    return(model_output)\n","\n","output = predict_with_network(input_data)\n","print(output)"]},{"cell_type":"markdown","id":"0ccd8b77-1e19-41c7-9677-b2218f06c262","metadata":{},"source":["## Optimizing a neural network with backward propagation  \n","The loss function measures the error. We want to minimize this function to get the best model parameters that fits our data.  \n","* Lower loss mean function means a better model  \n","* Goal: Find the weights that give the lowest value for the loss function  \n","* This is the Gradient Descent process.  \n","\n","**Backward propagation**\n","It is to calculate the slopes you need to optimize more complex deep learning models.   \n","* Just as forward propagation sends input data through the hidden layers and into the output layer, back propagation takes the error from the output layer and propagates it backward through the hidden layers, towards the input layer.  \n","* It calculates the necessary slopes sequentially from the weights closest to the prediction, through the hidden layers, eventually back to the weights coming from the inputs. We then use these slopes to update our weights as you've seen. \n","  \n","The process is the following:  \n","* Do forward propagation to calculate predictions and errors  \n","* Go one back layer at time \n","* Every weight feeds from some input node into some output node\n","* Gradients for weight is product of: \n","    1. Node value feeding into that weight\n","    2. Slope of loss function wrt node it feeds into\n","    3. Slope of activation function at the node it feeds into\n","* Need to also keep track of the slopes of the loss function respect node values\n","* Slope of node values are the sum of the slopes for all weights that come out of them"]},{"cell_type":"markdown","id":"85f05244-2db9-4443-b852-8b55d2b7d0e6","metadata":{},"source":["**Sthochastic gradient descent**  \n","* It is common only to calculate the slope on a only subset of the data (batch)\n","* Use a different batch of data to calculate the next update\n","* Star over from the beginning once all the data is used\n","* Each time through the training data is called epoch\n","* When slopes are calculated on one batch at time: stochastic gradient descent"]},{"cell_type":"markdown","id":"7906344a-dc5c-4207-8dcf-bb101767e1d6","metadata":{},"source":["## Building deep learning models in keras  \n"]},{"cell_type":"markdown","id":"bc8738ea-3f6c-4c0e-87ae-ee842066af19","metadata":{},"source":["**Model building steps**\n","* Specify architecture\n","* Compile\n","* Fit\n","* Predict\n","  \n"]},{"cell_type":"markdown","id":"837ae178-a7fc-4a44-ac8c-8b2c1e58485b","metadata":{},"source":["**Model specification**    \n","import numpy as np\n","from tensorflow.keras.layers import Dense  \n","from tensorflow.keras.models import Sequential  \n","  \n","predictors=np.loadtxt('predictors_data.csv',delimiter=',')  \n","n_cols=predictors.shape[1]  \n","  \n","model=Sequential()  \n","model.add(Dense(100,activation='relu',input_shape=(n_cols,)))  \n","model.add(Dense(100,activation='relu'))  \n","model.add(Dense(1))  "]},{"cell_type":"markdown","id":"6d8eb6f2-4caa-4175-8a0c-5bbebca69fe9","metadata":{},"source":["To start, you'll take the skeleton of a neural network and add a hidden layer and an output layer. You'll then fit that model and see Keras do the optimization so your model continually gets better.\n","\n","As a start, you'll predict workers wages based on characteristics like their industry, education and level of experience. You can find the dataset in a pandas DataFrame called df. For convenience, everything in df except for the target has been converted to a NumPy array called predictors. The target, wage_per_hour, is available as a NumPy array called target."]},{"cell_type":"code","execution_count":19,"id":"112f8f6a-f181-4846-8c94-357e3803fb56","metadata":{"executionTime":71,"lastSuccessfullyExecutedCode":"target=wages.wage_per_hour.values\npredictors=wages.drop(\"wage_per_hour\",axis=1).values  \n# Import necessary modules\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\n\n# Set up the model: model\nmodel = Sequential()\n\n# Add the first layer\nmodel.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n\n# Add the second layer\nmodel.add(Dense(32,activation='relu'))\n\n# Add the output layer\nmodel.add(Dense(1))"},"outputs":[],"source":["target=wages.wage_per_hour.values\n","predictors=wages.drop(\"wage_per_hour\",axis=1).values  \n","# Import necessary modules\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","\n","# Save the number of columns in predictors: n_cols\n","n_cols = predictors.shape[1]\n","\n","# Set up the model: model\n","model = Sequential()\n","\n","# Add the first layer\n","model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n","\n","# Add the second layer\n","model.add(Dense(32,activation='relu'))\n","\n","# Add the output layer\n","model.add(Dense(1))"]},{"cell_type":"markdown","id":"41b533cf-50be-42ad-b7a3-d4b8023dccca","metadata":{},"source":["**Compiling and fitting the model**  \n","The compile method has two important arguments for you to choose,  \n","* Specify the optimizer which controls the learning rate. \"Adam\" is usually a good choice, it adjusts the learning rate as it does gradient descent\n","* The loss function. mean_squared_error is the most common choice\n","  \n","After that...\n","Fitting!  \n","* Applying backpropagation and gradient descent with your data to update weights.\n","* Scaling all the data before fitting can ease optimization"]},{"cell_type":"code","execution_count":20,"id":"72da6bf4-097b-44b3-a4be-aefe118b323e","metadata":{"executionTime":502,"lastSuccessfullyExecutedCode":"# Import necessary modules\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n\n# Specify the model\nn_cols = predictors.shape[1]\nmodel = Sequential()\nmodel.add(Dense(50, activation='relu', input_shape = (n_cols,)))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1))\n\n# Compile the model\nmodel.compile(optimizer='adam',loss='mean_squared_error')\n\n# Verify that model contains information from compiling\nprint(\"Loss function: \" + model.loss)\n\nmodel.fit(predictors,target)"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss function: mean_squared_error\n","17/17 [==============================] - 0s 1ms/step - loss: 26.7871\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f80aeac7490>"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Import necessary modules\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","\n","# Specify the model\n","n_cols = predictors.shape[1]\n","model = Sequential()\n","model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1))\n","\n","# Compile the model\n","model.compile(optimizer='adam',loss='mean_squared_error')\n","\n","# Verify that model contains information from compiling\n","print(\"Loss function: \" + model.loss)\n","\n","model.fit(predictors,target)"]},{"cell_type":"markdown","id":"b382a08c-e0ea-46f0-b51e-5af5831ee3de","metadata":{},"source":["**Classification**\n","* 'categorical_crossestropy' as loss function.\n","* For this function a lower score is better. But better add metrics=['accuracy'] to compile step for easy to understand diagnostics.\n","* We need to modify the last layer, so it has a separate node for each potential outcome. Then use 'softmax' activation. It ensuere the predictions sum to 1, so they can be interpreted as probabilities"]},{"cell_type":"code","execution_count":21,"id":"5439b97a-a4a4-4b82-a395-5e38d6004c64","metadata":{"executionTime":119,"lastSuccessfullyExecutedCode":"print(titanic.info())"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 891 entries, 0 to 890\n","Data columns (total 11 columns):\n"," #   Column                     Non-Null Count  Dtype  \n","---  ------                     --------------  -----  \n"," 0   survived                   891 non-null    int64  \n"," 1   pclass                     891 non-null    int64  \n"," 2   age                        891 non-null    float64\n"," 3   sibsp                      891 non-null    int64  \n"," 4   parch                      891 non-null    int64  \n"," 5   fare                       891 non-null    float64\n"," 6   male                       891 non-null    int64  \n"," 7   age_was_missing            891 non-null    bool   \n"," 8   embarked_from_cherbourg    891 non-null    int64  \n"," 9   embarked_from_queenstown   891 non-null    int64  \n"," 10  embarked_from_southampton  891 non-null    int64  \n","dtypes: bool(1), float64(2), int64(8)\n","memory usage: 70.6 KB\n","None\n"]}],"source":["print(titanic.info())"]},{"cell_type":"code","execution_count":24,"id":"286f1999-6d5f-4089-860e-cb85724c96e1","metadata":{"executionTime":71,"lastSuccessfullyExecutedCode":"print(titanic.describe())"},"outputs":[{"name":"stdout","output_type":"stream","text":["         survived  ...  embarked_from_southampton\n","count  891.000000  ...                 891.000000\n","mean     0.383838  ...                   0.722783\n","std      0.486592  ...                   0.447876\n","min      0.000000  ...                   0.000000\n","25%      0.000000  ...                   0.000000\n","50%      0.000000  ...                   1.000000\n","75%      1.000000  ...                   1.000000\n","max      1.000000  ...                   1.000000\n","\n","[8 rows x 10 columns]\n"]}],"source":["print(titanic.describe())"]},{"cell_type":"code","execution_count":38,"id":"0c74b0b8-4dc9-4379-9c36-aac8c8fe748b","metadata":{"executionTime":82,"lastSuccessfullyExecutedCode":"print(to_categorical(titanic.survived).shape)\nprint(predictors.shape)\n"},"outputs":[{"name":"stdout","output_type":"stream","text":["(891, 2)\n","(891, 10)\n"]}],"source":["print(to_categorical(titanic.survived).shape)\n","print(predictors.shape)\n"]},{"cell_type":"code","execution_count":42,"id":"8c002b62-dc27-462a-b281-aeea0c500b4b","metadata":{"executionTime":355,"lastSuccessfullyExecutedCode":"# Import necessary modules\n# Import necessary modules\nimport numpy as np\npredictors=titanic.drop(\"survived\",axis=1).values.astype(np.float32)\ntarget = to_categorical(titanic.survived)\nn_cols=predictors.shape[1]\n\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\n\n# Convert the target to categorical: target\n\n\n# Set up the model\nmodel = Sequential()\n\n# Add the first layer\nmodel.add(Dense(32,activation='relu',input_shape=(n_cols,)))\n\n# Add the output layer\nmodel.add(Dense(2,activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer=\"sgd\",loss='categorical_crossentropy',metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(predictors,target)"},"outputs":[{"name":"stdout","output_type":"stream","text":["28/28 [==============================] - 0s 977us/step - loss: 4.1108 - accuracy: 0.5836\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f80ad5a0220>"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# Import necessary modules\n","# Import necessary modules\n","import numpy as np\n","predictors=titanic.drop(\"survived\",axis=1).values.astype(np.float32)\n","target = to_categorical(titanic.survived)\n","n_cols=predictors.shape[1]\n","\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","\n","# Convert the target to categorical: target\n","\n","\n","# Set up the model\n","model = Sequential()\n","\n","# Add the first layer\n","model.add(Dense(32,activation='relu',input_shape=(n_cols,)))\n","\n","# Add the output layer\n","model.add(Dense(2,activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer=\"sgd\",loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Fit the model\n","model.fit(predictors,target)"]},{"cell_type":"markdown","id":"a913f851-a6cd-4f3a-81c3-364cb3570926","metadata":{},"source":["Fantastic! This simple model is generating an accuracy of 60%!"]},{"cell_type":"markdown","id":"9852a3a1-f2c0-4315-a89b-a4481104c98d","metadata":{},"source":["## Using models  \n","* Save\n","* Reload\n","* Make predictions\n","  \n","from tensorflow.keras.models import load_model  \n","model.save('model_file.h5')  \n","my_model=load_model('model_file.h5')  \n","predictions=my_model.predict(data_to_predict_with)    \n","probability_true=predictions[:,1]  "]},{"cell_type":"markdown","id":"785b215d-06a9-44c9-8d20-a1b2fc80f3b8","metadata":{},"source":["Making predictions\n","\n","The trained network from your previous coding exercise is now stored as model. New data to make predictions is stored in a NumPy array as pred_data. Use model to make predictions on your new data."]},{"cell_type":"code","execution_count":6,"id":"08d8960e-2da8-48b7-ba81-6256668740f1","metadata":{"collapsed":false,"executionTime":119,"jupyter":{"outputs_hidden":false,"source_hidden":true},"lastSuccessfullyExecutedCode":"import numpy as np\npred_data=np.array([[  2.      ,  34.      ,   0.      ,   0.      ,  13.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  31.      ,   1.      ,   1.      ,  26.25    ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  11.      ,   1.      ,   2.      , 120.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,   0.42    ,   0.      ,   1.      ,   8.5167  ,\n          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  27.      ,   0.      ,   0.      ,   6.975   ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  31.      ,   0.      ,   0.      ,   7.775   ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  39.      ,   0.      ,   0.      ,   0.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  18.      ,   0.      ,   0.      ,   7.775   ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  39.      ,   0.      ,   0.      ,  13.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  33.      ,   1.      ,   0.      ,  53.1     ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  26.      ,   0.      ,   0.      ,   7.8875  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  39.      ,   0.      ,   0.      ,  24.15    ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  35.      ,   0.      ,   0.      ,  10.5     ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,   6.      ,   4.      ,   2.      ,  31.275   ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  30.5     ,   0.      ,   0.      ,   8.05    ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  29.699118,   0.      ,   0.      ,   0.      ,\n          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  23.      ,   0.      ,   0.      ,   7.925   ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  31.      ,   1.      ,   1.      ,  37.0042  ,\n          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  43.      ,   0.      ,   0.      ,   6.45    ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  10.      ,   3.      ,   2.      ,  27.9     ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  52.      ,   1.      ,   1.      ,  93.5     ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  27.      ,   0.      ,   0.      ,   8.6625  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  38.      ,   0.      ,   0.      ,   0.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  27.      ,   0.      ,   1.      ,  12.475   ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,   2.      ,   4.      ,   1.      ,  39.6875  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  29.699118,   0.      ,   0.      ,   6.95    ,\n          1.      ,   1.      ,   0.      ,   1.      ,   0.      ],\n       [  3.      ,  29.699118,   0.      ,   0.      ,  56.4958  ,\n          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,   1.      ,   0.      ,   2.      ,  37.0042  ,\n          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  29.699118,   0.      ,   0.      ,   7.75    ,\n          1.      ,   1.      ,   0.      ,   1.      ,   0.      ],\n       [  1.      ,  62.      ,   0.      ,   0.      ,  80.      ,\n          0.      ,   0.      ,   0.      ,   0.      ,   0.      ],\n       [  3.      ,  15.      ,   1.      ,   0.      ,  14.4542  ,\n          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  2.      ,   0.83    ,   1.      ,   1.      ,  18.75    ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  29.699118,   0.      ,   0.      ,   7.2292  ,\n          1.      ,   1.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  23.      ,   0.      ,   0.      ,   7.8542  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  18.      ,   0.      ,   0.      ,   8.3     ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  39.      ,   1.      ,   1.      ,  83.1583  ,\n          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  21.      ,   0.      ,   0.      ,   8.6625  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  29.699118,   0.      ,   0.      ,   8.05    ,\n          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  32.      ,   0.      ,   0.      ,  56.4958  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  29.699118,   0.      ,   0.      ,  29.7     ,\n          1.      ,   1.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  20.      ,   0.      ,   0.      ,   7.925   ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  16.      ,   0.      ,   0.      ,  10.5     ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  30.      ,   0.      ,   0.      ,  31.      ,\n          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  34.5     ,   0.      ,   0.      ,   6.4375  ,\n          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  17.      ,   0.      ,   0.      ,   8.6625  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  42.      ,   0.      ,   0.      ,   7.55    ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  29.699118,   8.      ,   2.      ,  69.55    ,\n          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  35.      ,   0.      ,   0.      ,   7.8958  ,\n          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  2.      ,  28.      ,   0.      ,   1.      ,  33.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  29.699118,   1.      ,   0.      ,  89.1042  ,\n          0.      ,   1.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,   4.      ,   4.      ,   2.      ,  31.275   ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  74.      ,   0.      ,   0.      ,   7.775   ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,   9.      ,   1.      ,   1.      ,  15.2458  ,\n          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  1.      ,  16.      ,   0.      ,   1.      ,  39.4     ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  44.      ,   1.      ,   0.      ,  26.      ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  18.      ,   0.      ,   1.      ,   9.35    ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  45.      ,   1.      ,   1.      , 164.8667  ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  51.      ,   0.      ,   0.      ,  26.55    ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  24.      ,   0.      ,   3.      ,  19.2583  ,\n          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  29.699118,   0.      ,   0.      ,   7.2292  ,\n          1.      ,   1.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  41.      ,   2.      ,   0.      ,  14.1083  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  21.      ,   1.      ,   0.      ,  11.5     ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  48.      ,   0.      ,   0.      ,  25.9292  ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  29.699118,   8.      ,   2.      ,  69.55    ,\n          0.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  24.      ,   0.      ,   0.      ,  13.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  42.      ,   0.      ,   0.      ,  13.      ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  27.      ,   1.      ,   0.      ,  13.8583  ,\n          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  1.      ,  31.      ,   0.      ,   0.      ,  50.4958  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  29.699118,   0.      ,   0.      ,   9.5     ,\n          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,   4.      ,   1.      ,   1.      ,  11.1333  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  26.      ,   0.      ,   0.      ,   7.8958  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  47.      ,   1.      ,   1.      ,  52.5542  ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  33.      ,   0.      ,   0.      ,   5.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  47.      ,   0.      ,   0.      ,   9.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  28.      ,   1.      ,   0.      ,  24.      ,\n          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  15.      ,   0.      ,   0.      ,   7.225   ,\n          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  20.      ,   0.      ,   0.      ,   9.8458  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  19.      ,   0.      ,   0.      ,   7.8958  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  29.699118,   0.      ,   0.      ,   7.8958  ,\n          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  56.      ,   0.      ,   1.      ,  83.1583  ,\n          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  2.      ,  25.      ,   0.      ,   1.      ,  26.      ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  33.      ,   0.      ,   0.      ,   7.8958  ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  22.      ,   0.      ,   0.      ,  10.5167  ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  28.      ,   0.      ,   0.      ,  10.5     ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  25.      ,   0.      ,   0.      ,   7.05    ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  39.      ,   0.      ,   5.      ,  29.125   ,\n          0.      ,   0.      ,   0.      ,   1.      ,   0.      ],\n       [  2.      ,  27.      ,   0.      ,   0.      ,  13.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  19.      ,   0.      ,   0.      ,  30.      ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  3.      ,  29.699118,   1.      ,   2.      ,  23.45    ,\n          0.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n       [  1.      ,  26.      ,   0.      ,   0.      ,  30.      ,\n          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n       [  3.      ,  32.      ,   0.      ,   0.      ,   7.75    ,\n          1.      ,   0.      ,   0.      ,   1.      ,   0.      ]],\n      dtype=np.float32)"},"outputs":[],"source":["import numpy as np\n","pred_data=np.array([[  2.      ,  34.      ,   0.      ,   0.      ,  13.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  31.      ,   1.      ,   1.      ,  26.25    ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  11.      ,   1.      ,   2.      , 120.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,   0.42    ,   0.      ,   1.      ,   8.5167  ,\n","          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  27.      ,   0.      ,   0.      ,   6.975   ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  31.      ,   0.      ,   0.      ,   7.775   ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  39.      ,   0.      ,   0.      ,   0.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  18.      ,   0.      ,   0.      ,   7.775   ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  39.      ,   0.      ,   0.      ,  13.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  33.      ,   1.      ,   0.      ,  53.1     ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  26.      ,   0.      ,   0.      ,   7.8875  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  39.      ,   0.      ,   0.      ,  24.15    ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  35.      ,   0.      ,   0.      ,  10.5     ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,   6.      ,   4.      ,   2.      ,  31.275   ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  30.5     ,   0.      ,   0.      ,   8.05    ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  29.699118,   0.      ,   0.      ,   0.      ,\n","          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  23.      ,   0.      ,   0.      ,   7.925   ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  31.      ,   1.      ,   1.      ,  37.0042  ,\n","          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  43.      ,   0.      ,   0.      ,   6.45    ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  10.      ,   3.      ,   2.      ,  27.9     ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  52.      ,   1.      ,   1.      ,  93.5     ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  27.      ,   0.      ,   0.      ,   8.6625  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  38.      ,   0.      ,   0.      ,   0.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  27.      ,   0.      ,   1.      ,  12.475   ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,   2.      ,   4.      ,   1.      ,  39.6875  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  29.699118,   0.      ,   0.      ,   6.95    ,\n","          1.      ,   1.      ,   0.      ,   1.      ,   0.      ],\n","       [  3.      ,  29.699118,   0.      ,   0.      ,  56.4958  ,\n","          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,   1.      ,   0.      ,   2.      ,  37.0042  ,\n","          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  29.699118,   0.      ,   0.      ,   7.75    ,\n","          1.      ,   1.      ,   0.      ,   1.      ,   0.      ],\n","       [  1.      ,  62.      ,   0.      ,   0.      ,  80.      ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   0.      ],\n","       [  3.      ,  15.      ,   1.      ,   0.      ,  14.4542  ,\n","          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  2.      ,   0.83    ,   1.      ,   1.      ,  18.75    ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  29.699118,   0.      ,   0.      ,   7.2292  ,\n","          1.      ,   1.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  23.      ,   0.      ,   0.      ,   7.8542  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  18.      ,   0.      ,   0.      ,   8.3     ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  39.      ,   1.      ,   1.      ,  83.1583  ,\n","          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  21.      ,   0.      ,   0.      ,   8.6625  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  29.699118,   0.      ,   0.      ,   8.05    ,\n","          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  32.      ,   0.      ,   0.      ,  56.4958  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  29.699118,   0.      ,   0.      ,  29.7     ,\n","          1.      ,   1.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  20.      ,   0.      ,   0.      ,   7.925   ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  16.      ,   0.      ,   0.      ,  10.5     ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  30.      ,   0.      ,   0.      ,  31.      ,\n","          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  34.5     ,   0.      ,   0.      ,   6.4375  ,\n","          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  17.      ,   0.      ,   0.      ,   8.6625  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  42.      ,   0.      ,   0.      ,   7.55    ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  29.699118,   8.      ,   2.      ,  69.55    ,\n","          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  35.      ,   0.      ,   0.      ,   7.8958  ,\n","          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  2.      ,  28.      ,   0.      ,   1.      ,  33.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  29.699118,   1.      ,   0.      ,  89.1042  ,\n","          0.      ,   1.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,   4.      ,   4.      ,   2.      ,  31.275   ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  74.      ,   0.      ,   0.      ,   7.775   ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,   9.      ,   1.      ,   1.      ,  15.2458  ,\n","          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  1.      ,  16.      ,   0.      ,   1.      ,  39.4     ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  44.      ,   1.      ,   0.      ,  26.      ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  18.      ,   0.      ,   1.      ,   9.35    ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  45.      ,   1.      ,   1.      , 164.8667  ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  51.      ,   0.      ,   0.      ,  26.55    ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  24.      ,   0.      ,   3.      ,  19.2583  ,\n","          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  29.699118,   0.      ,   0.      ,   7.2292  ,\n","          1.      ,   1.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  41.      ,   2.      ,   0.      ,  14.1083  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  21.      ,   1.      ,   0.      ,  11.5     ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  48.      ,   0.      ,   0.      ,  25.9292  ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  29.699118,   8.      ,   2.      ,  69.55    ,\n","          0.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  24.      ,   0.      ,   0.      ,  13.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  42.      ,   0.      ,   0.      ,  13.      ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  27.      ,   1.      ,   0.      ,  13.8583  ,\n","          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  1.      ,  31.      ,   0.      ,   0.      ,  50.4958  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  29.699118,   0.      ,   0.      ,   9.5     ,\n","          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,   4.      ,   1.      ,   1.      ,  11.1333  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  26.      ,   0.      ,   0.      ,   7.8958  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  47.      ,   1.      ,   1.      ,  52.5542  ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  33.      ,   0.      ,   0.      ,   5.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  47.      ,   0.      ,   0.      ,   9.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  28.      ,   1.      ,   0.      ,  24.      ,\n","          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  15.      ,   0.      ,   0.      ,   7.225   ,\n","          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  20.      ,   0.      ,   0.      ,   9.8458  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  19.      ,   0.      ,   0.      ,   7.8958  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  29.699118,   0.      ,   0.      ,   7.8958  ,\n","          1.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  56.      ,   0.      ,   1.      ,  83.1583  ,\n","          0.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  2.      ,  25.      ,   0.      ,   1.      ,  26.      ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  33.      ,   0.      ,   0.      ,   7.8958  ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  22.      ,   0.      ,   0.      ,  10.5167  ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  28.      ,   0.      ,   0.      ,  10.5     ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  25.      ,   0.      ,   0.      ,   7.05    ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  39.      ,   0.      ,   5.      ,  29.125   ,\n","          0.      ,   0.      ,   0.      ,   1.      ,   0.      ],\n","       [  2.      ,  27.      ,   0.      ,   0.      ,  13.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  19.      ,   0.      ,   0.      ,  30.      ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  3.      ,  29.699118,   1.      ,   2.      ,  23.45    ,\n","          0.      ,   1.      ,   0.      ,   0.      ,   1.      ],\n","       [  1.      ,  26.      ,   0.      ,   0.      ,  30.      ,\n","          1.      ,   0.      ,   1.      ,   0.      ,   0.      ],\n","       [  3.      ,  32.      ,   0.      ,   0.      ,   7.75    ,\n","          1.      ,   0.      ,   0.      ,   1.      ,   0.      ]],\n","      dtype=np.float32)"]},{"cell_type":"code","execution_count":7,"id":"36c5b2cf-0f16-47b6-85e4-9af0ac0644e9","metadata":{"executionTime":566,"lastSuccessfullyExecutedCode":"# Specify, compile, and fit the model\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\n\nimport numpy as np\npredictors=titanic.drop(\"survived\",axis=1).values.astype(np.float32)\ntarget = to_categorical(titanic.survived)\nn_cols=predictors.shape[1]\n\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_shape = (n_cols,)))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer='sgd', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])\nmodel.fit(predictors, target)\n\n# Calculate predictions: predictions\npredictions = model.predict(pred_data)\n\n# Calculate predicted probability of survival: predicted_prob_true\npredicted_prob_true = predictions[:,1]\n\n# Print predicted_prob_true\nprint(predicted_prob_true)"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-02-10 20:44:10.326159: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2023-02-10 20:44:10.326182: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n","2023-02-10 20:44:10.326200: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (81571eb2-e43a-4af8-9196-06645585934b): /proc/driver/nvidia/version does not exist\n","2023-02-10 20:44:10.326432: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]},{"name":"stdout","output_type":"stream","text":["28/28 [==============================] - 0s 940us/step - loss: 3.8978 - accuracy: 0.5802\n","3/3 [==============================] - 0s 1ms/step\n","[0.89903545 0.9873754  0.99999994 0.7256456  0.79595697 0.82457614\n"," 0.80100083 0.7893369  0.9042819  0.9999624  0.80296797 0.978068\n"," 0.86450136 0.99515736 0.82606095 0.7913013  0.8218883  0.99801016\n"," 0.8548541  0.989624   0.99999994 0.8162313  0.7948997  0.88613206\n"," 0.99870193 0.80256206 0.99998254 0.9985419  0.8136764  0.9999996\n"," 0.8910401  0.94298625 0.8485126  0.78223234 0.7591281  0.99999994\n"," 0.77944607 0.86215436 0.9999687  0.99568236 0.7610588  0.8124938\n"," 0.9965125  0.8162964  0.7643757  0.86331254 0.9999985  0.8370561\n"," 0.9959861  0.99999994 0.99331737 0.9416565  0.9082757  0.9995155\n"," 0.98648673 0.8071901  0.99999994 0.98896915 0.94325733 0.8485126\n"," 0.90207905 0.8234772  0.9899656  0.99999887 0.8791681  0.9300592\n"," 0.89547133 0.9999164  0.8768835  0.77369344 0.8030436  0.9999313\n"," 0.77110434 0.89249355 0.98243195 0.75636524 0.80302304 0.75389034\n"," 0.8605038  0.9999998  0.9886588  0.83422434 0.8564964  0.84740454\n"," 0.7876065  0.983409   0.8858254  0.9964536  0.98005533 0.99460524\n"," 0.7758727 ]\n"]}],"source":["# Specify, compile, and fit the model\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","\n","import numpy as np\n","predictors=titanic.drop(\"survived\",axis=1).values.astype(np.float32)\n","target = to_categorical(titanic.survived)\n","n_cols=predictors.shape[1]\n","\n","\n","model = Sequential()\n","model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\n","model.add(Dense(2, activation='softmax'))\n","model.compile(optimizer='sgd', \n","              loss='categorical_crossentropy', \n","              metrics=['accuracy'])\n","model.fit(predictors, target)\n","\n","# Calculate predictions: predictions\n","predictions = model.predict(pred_data)\n","\n","# Calculate predicted probability of survival: predicted_prob_true\n","predicted_prob_true = predictions[:,1]\n","\n","# Print predicted_prob_true\n","print(predicted_prob_true)"]},{"cell_type":"markdown","id":"d2da6325-c7d5-4321-a2ba-2ec75940dff7","metadata":{},"source":["## Fine-tuning keras model"]},{"cell_type":"markdown","id":"0ed1f21a-8df8-4ca0-8cd1-94bfae809ea7","metadata":{},"source":["* Updates may not improve the model meaningfully\n","* Updates too small (if learning rate is small) or too large (if learning rate is high)\n","  \n","* The easiest way to see the effect of different learning rates is to use the simplest optimizer.  \n","* The *dying neuron problem*: It occurs when a neuron takes a value less than 0 for all rows of your data. Recall that ReLu activation gets 0 when input is a negative value...  \n","* Using the sigmoid function. The values outside the middle are relatively flat or had small slopes. Then many layers have very small slopes.\n","* In deep networks, updates back to propagation were close to zero. This is called the *vanishing gradient problem*.   "]},{"cell_type":"markdown","id":"9a41ed0e-606d-4a0d-84ad-e92b8e25ed85","metadata":{},"source":["Changing optimization parameters\n","\n","It's time to get your hands dirty with optimization. You'll now try optimizing a model at a very low learning rate, a very high learning rate, and a \"just right\" learning rate. You'll want to look at the results after running this exercise, remembering that a low value for the loss function is good."]},{"cell_type":"code","execution_count":14,"id":"94cec83d-08f4-4d4b-8879-3cfc0e5d7ba6","metadata":{"executionTime":1037,"lastSuccessfullyExecutedCode":"#Stochastic gradient descent  \nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import SGD\n\npredictors=titanic.drop(\"survived\",axis=1).values.astype(np.float32)\ntarget = to_categorical(titanic.survived)\nn_cols=predictors.shape[1]\n\ndef get_new_model(input_shape):\n    model=Sequential()\n    input_shape=(predictors.shape[1],)\n    model.add(Dense(100,activation=\"relu\",input_shape=input_shape))\n    model.add(Dense(100,activation='relu'))\n    model.add(Dense(2,activation='softmax'))\n    return(model)\n\nlr_to_test=[0.000001,0.01,1]\ninput_shape=(predictors.shape[1],)\n\nfor lr in lr_to_test:\n    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n    model=get_new_model(input_shape)\n    my_optimizer=SGD(lr=lr)\n    model.compile(optimizer=my_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n    # Fit the model\n    model.fit(predictors,target)\n    "},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Testing model with learning rate: 0.000001\n","\n","28/28 [==============================] - 0s 1ms/step - loss: 1.5098 - accuracy: 0.3883\n","\n","\n","Testing model with learning rate: 0.010000\n","\n","28/28 [==============================] - 0s 1ms/step - loss: 1.3469 - accuracy: 0.6285\n","\n","\n","Testing model with learning rate: 1.000000\n","\n","28/28 [==============================] - 0s 1ms/step - loss: 5318389970354831360.0000 - accuracy: 0.5881\n"]}],"source":["#Stochastic gradient descent  \n","import pandas as pd\n","import numpy as np\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import SGD\n","\n","predictors=titanic.drop(\"survived\",axis=1).values.astype(np.float32)\n","target = to_categorical(titanic.survived)\n","n_cols=predictors.shape[1]\n","\n","def get_new_model(input_shape):\n","    model=Sequential()\n","    input_shape=(predictors.shape[1],)\n","    model.add(Dense(100,activation=\"relu\",input_shape=input_shape))\n","    model.add(Dense(100,activation='relu'))\n","    model.add(Dense(2,activation='softmax'))\n","    return(model)\n","\n","lr_to_test=[0.000001,0.01,1]\n","input_shape=(predictors.shape[1],)\n","\n","for lr in lr_to_test:\n","    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n","    model=get_new_model(input_shape)\n","    my_optimizer=SGD(lr=lr)\n","    model.compile(optimizer=my_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n","    # Fit the model\n","    model.fit(predictors,target)\n","    "]},{"cell_type":"markdown","id":"dfab25b7-99d4-4a5b-9675-e8c2ea0f7ee9","metadata":{},"source":["## Model validation  \n","* We use validation data to test model performance\n","* Commonly use validation split rather than cross validation since we employ large datasets  \n","* Single validation score is based on large amount of data, and is reliable  \n","  \n","We specify the split using the keyword argument validation_split when calling the fit method.  Suppose we have a classification problem:     \n","model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])  \n","model.fit(predictors,target,validation_split=0.3)  \n","  \n","Our goal is to have the best validation score possible, so we should keep training while validation score is improving, and then stop training when the validation score isn't improving. We do this with something called \"early stopping.\" We can use early stopping with only some small changes to the code. \n","  \n","**Early stopping**  \n","from tensorflow.keras.callbacks import EarlyStopping  \n","early_stopping_monitor=EarlyStopping(patience=2)  \n","model.fit(predictors,target,validation_split=0.3,epochs=20,callbacks=[early_stopping_monitor])  "]},{"cell_type":"markdown","id":"41057ca7-ac4e-4e8f-9f8b-b14f35b75b2e","metadata":{},"source":["Evaluating model accuracy on validation dataset\n","\n","Now it's your turn to monitor model accuracy with a validation data set. A model definition has been provided as model. Your job is to add the code to compile it and then fit it. You'll check the validation score in each epoch."]},{"cell_type":"code","execution_count":2,"id":"c3cc6131-3521-47e5-80b4-4df6f75cd024","metadata":{"executionTime":2894,"lastSuccessfullyExecutedCode":"#Stochastic gradient descent  \nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import SGD\n\npredictors=titanic.drop(\"survived\",axis=1).values.astype(np.float32)\ntarget = to_categorical(titanic.survived)\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols,)\n\n# Specify the model\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape = input_shape))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n# Fit the model\nhist = model.fit(predictors,target,validation_split=0.3)"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-02-11 02:53:29.986269: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2023-02-11 02:53:29.986307: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","2023-02-11 02:53:31.831131: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2023-02-11 02:53:31.831154: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n","2023-02-11 02:53:31.831174: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9847d3a7-57b4-404b-a877-78247bda865a): /proc/driver/nvidia/version does not exist\n","2023-02-11 02:53:31.831399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]},{"name":"stdout","output_type":"stream","text":["20/20 [==============================] - 1s 10ms/step - loss: 0.8313 - accuracy: 0.6533 - val_loss: 0.6444 - val_accuracy: 0.6679\n"]}],"source":["#Stochastic gradient descent  \n","import pandas as pd\n","import numpy as np\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import SGD\n","\n","predictors=titanic.drop(\"survived\",axis=1).values.astype(np.float32)\n","target = to_categorical(titanic.survived)\n","\n","# Save the number of columns in predictors: n_cols\n","n_cols = predictors.shape[1]\n","input_shape = (n_cols,)\n","\n","# Specify the model\n","model = Sequential()\n","model.add(Dense(100, activation='relu', input_shape = input_shape))\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(2, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Fit the model\n","hist = model.fit(predictors,target,validation_split=0.3)"]},{"cell_type":"markdown","id":"dcdd59e3-7db0-4019-9061-79661e04cb7d","metadata":{},"source":["Early stopping: Optimizing the optimization\n","\n","Now that you know how to monitor your model performance throughout optimization, you can use early stopping to stop optimization when it isn't helping any more. Since the optimization stops automatically when it isn't helping, you can also set a high value for epochs in your call to .fit()"]},{"cell_type":"code","execution_count":5,"id":"0aa2b2e0-a9c6-4b92-b4bf-0f10234f9512","metadata":{"executionTime":1562,"lastSuccessfullyExecutedCode":"# Import EarlyStopping\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import SGD\n\npredictors=titanic.drop(\"survived\",axis=1).values.astype(np.float32)\ntarget = to_categorical(titanic.survived)\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols,)\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols,)\n\n# Specify the model\nmodel_1 = Sequential()\nmodel_1.add(Dense(100, activation='relu', input_shape = input_shape))\nmodel_1.add(Dense(100, activation='relu'))\nmodel_1.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n# Define early_stopping_monitor\nearly_stopping_monitor = EarlyStopping(patience=2)\n\n# Fit the model\nmodel_1.fit(predictors,target,validation_split=0.3,epochs=30,callbacks=[early_stopping_monitor])"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","20/20 [==============================] - 0s 9ms/step - loss: 1.3659 - accuracy: 0.5811 - val_loss: 0.6338 - val_accuracy: 0.7239\n","Epoch 2/30\n","20/20 [==============================] - 0s 3ms/step - loss: 0.7191 - accuracy: 0.6372 - val_loss: 0.5753 - val_accuracy: 0.7164\n","Epoch 3/30\n","20/20 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.6485 - val_loss: 0.6511 - val_accuracy: 0.6716\n","Epoch 4/30\n","20/20 [==============================] - 0s 3ms/step - loss: 0.6348 - accuracy: 0.6533 - val_loss: 0.5612 - val_accuracy: 0.7276\n","Epoch 5/30\n","20/20 [==============================] - 0s 3ms/step - loss: 0.5955 - accuracy: 0.6806 - val_loss: 0.5719 - val_accuracy: 0.6940\n","Epoch 6/30\n","20/20 [==============================] - 0s 3ms/step - loss: 0.6062 - accuracy: 0.6629 - val_loss: 0.5233 - val_accuracy: 0.7276\n","Epoch 7/30\n","20/20 [==============================] - 0s 4ms/step - loss: 0.6830 - accuracy: 0.6613 - val_loss: 0.5208 - val_accuracy: 0.7276\n","Epoch 8/30\n","20/20 [==============================] - 0s 3ms/step - loss: 0.6220 - accuracy: 0.7014 - val_loss: 0.5289 - val_accuracy: 0.7761\n","Epoch 9/30\n","20/20 [==============================] - 0s 3ms/step - loss: 0.5584 - accuracy: 0.6998 - val_loss: 0.5046 - val_accuracy: 0.7687\n","Epoch 10/30\n","20/20 [==============================] - 0s 3ms/step - loss: 0.5840 - accuracy: 0.7095 - val_loss: 0.5194 - val_accuracy: 0.7239\n","Epoch 11/30\n","20/20 [==============================] - 0s 3ms/step - loss: 0.5806 - accuracy: 0.7127 - val_loss: 0.5597 - val_accuracy: 0.7612\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fbe58773040>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Import EarlyStopping\n","import pandas as pd\n","import numpy as np\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import SGD\n","\n","predictors=titanic.drop(\"survived\",axis=1).values.astype(np.float32)\n","target = to_categorical(titanic.survived)\n","\n","# Save the number of columns in predictors: n_cols\n","n_cols = predictors.shape[1]\n","input_shape = (n_cols,)\n","\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Save the number of columns in predictors: n_cols\n","n_cols = predictors.shape[1]\n","input_shape = (n_cols,)\n","\n","# Specify the model\n","model_1 = Sequential()\n","model_1.add(Dense(100, activation='relu', input_shape = input_shape))\n","model_1.add(Dense(100, activation='relu'))\n","model_1.add(Dense(2, activation='softmax'))\n","\n","# Compile the model\n","model_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Define early_stopping_monitor\n","early_stopping_monitor = EarlyStopping(patience=2)\n","\n","# Fit the model\n","model_1.fit(predictors,target,validation_split=0.3,epochs=30,callbacks=[early_stopping_monitor])"]},{"cell_type":"markdown","id":"8b320149-b92d-40d8-8886-1b3235528d55","metadata":{},"source":["Experimenting with wider networks\n","\n","Now you know everything you need to begin experimenting with different models!\n","\n","A model called model_1 has been pre-loaded. You can see a summary of this model printed in the IPython Shell. This is a relatively small network, with only 10 units in each hidden layer.\n","\n","In this exercise you'll create a new model called model_2 which is similar to model_1, except it has 100 units in each hidden layer.\n","\n","After you create model_2, both models will be fitted, and a graph showing both models loss score at each epoch will be shown. We added the argument verbose=False in the fitting commands to print out fewer updates, since you will look at these graphically instead of as text.\n","\n","Because you are fitting two models, it will take a moment to see the outputs after you hit run, so be patient."]},{"cell_type":"code","execution_count":26,"id":"3209994d-ef69-4fc1-a54d-ebb3afc6eca5","metadata":{"executionTime":1916,"lastSuccessfullyExecutedCode":"# Define early_stopping_monitor\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping_monitor = EarlyStopping(patience=2)\n\n# Create the new model: model_2\nmodel_2 = Sequential()\n\n# Add the first and second layers\nmodel_2.add(Dense(100, activation='relu', input_shape=input_shape))\nmodel_2.add(Dense(100,activation='relu'))\n\n# Add the output layer\nmodel_2.add(Dense(2,activation='softmax'))\n\n# Compile model_2\nmodel_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n# Fit model_1\nmodel_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n\n# Fit model_2\nmodel_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n\n# Create the plot\nplt.plot(model_1_training.history['val_loss'], 'r')\nplt.plot(model_2_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.legend(loc=\"upper left\")\nplt.show()"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Define early_stopping_monitor\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","early_stopping_monitor = EarlyStopping(patience=2)\n","\n","# Create the new model: model_2\n","model_2 = Sequential()\n","\n","# Add the first and second layers\n","model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n","model_2.add(Dense(100,activation='relu'))\n","\n","# Add the output layer\n","model_2.add(Dense(2,activation='softmax'))\n","\n","# Compile model_2\n","model_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Fit model_1\n","model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n","\n","# Fit model_2\n","model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n","\n","# Create the plot\n","plt.plot(model_1_training.history['val_loss'], 'r')\n","plt.plot(model_2_training.history['val_loss'], 'b')\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation score')\n","plt.legend(loc=\"upper left\")\n","plt.show()"]},{"cell_type":"markdown","id":"b0a1e731-bd06-41b4-bbfb-1653cc955df4","metadata":{},"source":["Adding layers to a network\n","\n","You've seen how to experiment with wider networks. In this exercise, you'll try a deeper network (more hidden layers).\n","\n","Once again, you have a baseline model called model_1 as a starting point. It has 1 hidden layer, with 10 units. You can see a summary of that model's structure printed out. You will create a similar network with 3 hidden layers (still keeping 10 units in each layer)."]},{"cell_type":"code","execution_count":null,"id":"5764f84c-363f-4a0f-98ce-09db6fcaf07f","metadata":{"executionTime":97,"lastSuccessfullyExecutedCode":"# The input shape to use in the first hidden layer\ninput_shape = (n_cols,)\n\n# Create the new model: model_2\nmodel_2 = Sequential()\n\n# Add the first, second, and third hidden layers\nmodel_2.add(Dense(10,activation='relu',input_shape=input_shape))\nmodel_2.add(Dense(10,activation='relu'))\nmodel_2.add(Dense(10,activation='relu'))\n\n# Add the output layer\nmodel_2.add(Dense(2,activation='softmax'))\n\n# Compile model_2\nmodel_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n# Fit model 1\nmodel_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.4, verbose=False)\n\n# Fit model 2\nmodel_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.4, verbose=False)\n\n# Create the plot\nplt.plot(model_1_training.history['val_loss'], 'r',label='model_1', model_2_training.history['val_loss'], 'b',label='model_2')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABe6ElEQVR4nO3dd1iTZ9sG8DMgSxkqyBBRqXsiLoq7FXfta5faWrHOatVqsbbaOt62Vr8Oqx1Wq3W11bpbbd3itigKYrXiHlAVnGwFSe7vj/tNIDIkmORJwvk7jhwmD0+SK4DkzD1VQggBIiIiIhthp3QBRERERMbEcENEREQ2heGGiIiIbArDDREREdkUhhsiIiKyKQw3REREZFMYboiIiMimlFO6AHPTaDS4fv063NzcoFKplC6HiIiISkAIgfT0dFStWhV2dsW3zZS5cHP9+nUEBAQoXQYRERGVQmJiIqpVq1bsOWUu3Li5uQGQ3xx3d3eFqyEiIqKSSEtLQ0BAgO59vDhlLtxou6Lc3d0ZboiIiKxMSYaUcEAxERER2RSGGyIiIrIpDDdERERkU8rcmBsiIip71Go1Hj58qHQZ9BiOjo6PneZdEgw3RERks4QQSEpKQkpKitKlUAnY2dkhMDAQjo6OT/Q4DDdERGSztMHG29sb5cuX5+KtFky7yO6NGzdQvXr1J/pZMdwQEZFNUqvVumDj6empdDlUAlWqVMH169eRm5sLBweHUj8OBxQTEZFN0o6xKV++vMKVUElpu6PUavUTPQ7DDRER2TR2RVkPY/2sGG6IiIjIpjDcEBERkU1huCEiIiqjOnXqhPHjx5f4/GXLlqFixYomq8dYGG6MKC0NiIlRugoiIiLl3bhxA6+99hrq1q0LOzs7g0LUk2K4MZKYGMDTE+jVC9BolK6GiIhIWdnZ2ahSpQqmTJmCoKAgsz43w42RNG4MODkByclAXJzS1RARUaGEADIzlbkIUeIyO3XqhLFjx2L8+PGoVKkSfHx8sGjRImRmZmLw4MFwc3ND7dq1sXXrVt199u3bh9atW8PJyQl+fn6YNGkScnNzdV/PzMxEeHg4XF1d4efnh9mzZxd43uzsbLz77rvw9/dHhQoVEBISgr1795bqW12zZk18/fXXCA8Ph4eHR6keo7QYbozEyQno3Flez/e7RkREliQrC3B1VeaSlWVQqcuXL4eXlxeio6MxduxYjBo1Cq+88gratGmD2NhYdO3aFQMHDkRWVhauXbuGnj17olWrVjhx4gTmz5+PxYsXY8aMGbrHmzhxIvbt24eNGzdix44d2Lt3L2JjY/Wec8yYMYiKisKqVavw999/45VXXkH37t1x/vx5o3z7zUaUMampqQKASE1NNfpjf/+9EIAQ7doZ/aGJiMhA9+/fF6dPnxb379/PO5iRIf9QK3HJyChx7R07dhTt8r2Z5ObmigoVKoiBAwfqjt24cUMAEFFRUeKDDz4Q9erVExqNRvf1efPmCVdXV6FWq0V6erpwdHQUa9as0X39zp07wsXFRYwbN04IIcTVq1eFvb29uHbtml4tnTt3FpMnTxZCCLF06VLh4eFR4teR//Von6c4hf7M/seQ929uv2BEPXrIf6OigJQUwAoGlBMRlS3lywMZGco9twGaNm2qu25vbw9PT080adJEd8zHxwcAcPPmTcTHxyM0NFRvEby2bdsiIyMD//77L+7du4ecnByEhITovl65cmXUq1dPd/vkyZNQq9WoW7euXh3Z2dlWt30Fw40R1awJ1K8PnDkD7NoFvPyy0hUREZEelQqoUEHpKkrk0b2VVCqV3jFtkNEYaRZLRkYG7O3tERMTA3t7e72vubq6GuU5zIVjboyse3f5L8fdEBGRuTRo0ABRUVEQ+QYtHzp0CG5ubqhWrRpq1aoFBwcHHDlyRPf1e/fu4dy5c7rbwcHBUKvVuHnzJmrXrq138fX1NevreVIMN0am7Zrats2ggfFERESl9tZbbyExMRFjx47FmTNnsHHjRkyfPh0RERGws7ODq6srhg4diokTJ2L37t04deoU3njjDdjZ5cWAunXrYsCAAQgPD8eGDRtw+fJlREdHY9asWdi8eXOp6oqLi0NcXBwyMjJw69YtxMXF4fTp08Z62UVit5SRdegAuLgA168DJ08C+bpMiYiITMLf3x9btmzBxIkTERQUhMqVK2Po0KGYMmWK7pwvvvgCGRkZ6N27N9zc3DBhwgSkpqbqPc7SpUsxY8YMTJgwAdeuXYOXlxeefvppPPfcc6WqKzg4WHc9JiYGK1euRI0aNXDlypVSPV5JqYQoW+0LaWlp8PDwQGpqKtzd3U3yHL16AVu2AJ99Brz3nkmegoiIHuPBgwe4fPkyAgMD4ezsrHQ5VALF/cwMef9WtFtq//796N27N6pWrQqVSoXff/+92POVXMrZEBx3Q0REpBxFw01mZiaCgoIwb968Ep2v5FLOhtCOuzl4EEhPV7YWIiIiU2jUqBFcXV0LvaxYsULR2hQdc9OjRw/00CaBEtAu5QwAS5YsKdF9srOzkZ2drbudlpZmWJGlULs2UKsWcPEiEBkJ9Olj8qckIiIyqy1btuDhw4eFfk27Bo9SbH5A8axZs/DRRx+Z/Xl79AC++052TTHcEBGRralRo4bSJRTJ5qeCT548GampqbpLYmKiWZ6XU8KJiIiUYfMtN05OTnBycjL783bqJDfTTEgA4uOBhg3NXgIREVGZZPMtN0opXx7o2FFe37ZN2VqIiIjKEoYbE+KUcCIiIvNTNNxkZGTolmYGgMuXLyMuLg4JCQkA5HiZ8PBwvfsotZRzaWjH3ezfD2RmKlsLERFRWaFouDl27BiCg4N1yzNHREQgODgY06ZNAyAX7dMGHS3t+dplnIODg9GzZ0+z114S9eoBNWoAOTnAnj1KV0NERKSvU6dOBi2Iu2zZMlSsWNFk9RiLouGmU6dOEEIUuCxbtgyA/Cbu3btX7z6FnW/qPSpKS6XSnzVFRERUVmzYsAFdunRBlSpV4O7ujtDQUGzfvt0sz80xNyaWf9wNp4QTEVFZsX//fnTp0gVbtmxBTEwMnnnmGfTu3RvHjx83+XMz3JjYs88CDg7ApUvA+fNKV0NEVLYJIcdAKnEx5ANup06dMHbsWIwfPx6VKlWCj48PFi1ahMzMTAwePBhubm6oXbs2tuabsbJv3z60bt0aTk5O8PPzw6RJk5Cbm6v7emZmJsLDw+Hq6go/Pz/Mnj27wPNmZ2fj3Xffhb+/PypUqICQkJACPSglNXfuXLz33nto1aoV6tSpg5kzZ6JOnTr4448/SvV4hmC4MTE3N6B9e3mdXVNERMrKygJcXZW5ZGUZVuvy5cvh5eWF6OhojB07FqNGjcIrr7yCNm3aIDY2Fl27dsXAgQORlZWFa9euoWfPnmjVqhVOnDiB+fPnY/HixZgxY4bu8SZOnIh9+/Zh48aN2LFjB/bu3YvY2Fi95xwzZgyioqKwatUq/P3333jllVfQvXt3nDfCp3ONRoP09HRUrlz5iR/rsUQZk5qaKgCI1NRUsz3n558LAQjRvbvZnpKIqMy7f/++OH36tLh//77uWEaG/HusxCUjo+S1d+zYUbRr1053Ozc3V1SoUEEMHDhQd+zGjRsCgIiKihIffPCBqFevntBoNLqvz5s3T7i6ugq1Wi3S09OFo6OjWLNmje7rd+7cES4uLmLcuHFCCCGuXr0q7O3txbVr1/Rq6dy5s5g8ebIQQoilS5cKDw+Pkr+QfD777DNRqVIlkZycXOQ5hf3MtAx5/7b5FYotQY8ewHvvAXv3AvfvAy4uSldERFQ2lS8PZGQo99yGaNq0qe66vb09PD090aRJE90x7eaUN2/eRHx8PEJDQ6FSqXRfb9u2LTIyMvDvv//i3r17yMnJQUhIiO7rlStXRr169XS3T548CbVajbp16+rVkZ2dDU9PT8OKf8TKlSvx0UcfYePGjfD29n6ixyoJhhszaNQI8PcHrl0D9u3LG2RMRETmpVIBFSooXUXJODg46N1WqVR6x7RBRqPRGOX5MjIyYG9vj5iYGNjb2+t9zdXVtdSPu2rVKgwbNgxr165FWFjYk5ZZIhxzYwacEk5ERKbUoEEDREVFQeQbtXzo0CG4ubmhWrVqqFWrFhwcHHDkyBHd1+/du4dz587pbgcHB0OtVuPmzZuoXbu23sXX17dUdf36668YPHgwfv31V/Tq1av0L9BADDdmwq0YiIjIVN566y0kJiZi7NixOHPmDDZu3Ijp06cjIiICdnZ2cHV1xdChQzFx4kTs3r0bp06dwhtvvAE7u7wYULduXQwYMADh4eHYsGEDLl++jOjoaMyaNQubN282uKaVK1ciPDwcs2fPRkhICJKSkpCUlITU1FRjvvRCMdyYSVgYYG8PnDsnp4UTEREZi7+/P7Zs2YLo6GgEBQVh5MiRGDp0KKZMmaI754svvkD79u3Ru3dvhIWFoV27dmjRooXe4yxduhTh4eGYMGEC6tWrhz59+uDo0aOoXr26wTUtXLgQubm5GD16NPz8/HSXcePGPfHrfRyVyN+GVQakpaXBw8MDqampcHd3N+tzd+gAHDgAzJsHvPWWWZ+aiKjMefDgAS5fvozAwEA4OzsrXQ6VQHE/M0Pev9lyY0bacTfsmiIiIjIdhhsz0oab3buB7GxlayEiInoSjRo1gqura6GXFStWKFobp4KbUVAQ4OsLJCXJ7ikzzYgjIiIyui1btuDhw4eFfk27Bo9SGG7MSKWSs6aWLZNTwhluiIjIWtWoUUPpEorEbikz45RwIiLzMtYid2R6xprjxJYbM+vSBbCzA06fBhISgFLMriMiohJwdHSEnZ0drl+/jipVqsDR0VFvewKyLEII3Lp1q8BKzKXBcGNmlSsDISFAVJTsmhoxQumKiIhsk52dHQIDA3Hjxg1cv35d6XKoBFQqFapVq1Zg+wdDMdwooEcPhhsiInNwdHRE9erVkZubC7VarXQ59BgODg5PHGwAhhtFdO8OTJsG7NoF5OQAjo5KV0REZLu03RxP2tVB1oMDihXQogVQpQqQng789ZfS1RAREdkWhhsF2NkB3brJ69wlnIiIyLgYbhTCKeFERESmwXCjkK5d5aJ+f/8NcBA/ERGR8TDcKKRKFaBlS3mdXVNERETGw3CjIO1Gmgw3RERExsNwoyDtuJudO4HcXGVrISIishUMNwpq3RqoVAlISQGOHFG6GiIiItvAcKMge3s5sBjgrCkiIiJjYbhRmHbcDcMNERGRcTDcKEy7mF9sLJCcrGwtREREtoDhRmG+vkBwsLy+fbuytRAREdkChhsLwCnhRERExsNwYwG0U8K3bwfUamVrISIisnYMNxYgNBTw8ADu3gWOHVO6GiIiIuvGcGMBypUDwsLkdc6aIiIiejIMNxaC426IiIiMg+HGQminhEdHA7dvK1sLERGRNVM03Ozfvx+9e/dG1apVoVKp8Pvvvz/2Pnv37kXz5s3h5OSE2rVrY9myZSav0xyqVQOaNAGEkHtNERERUekoGm4yMzMRFBSEefPmlej8y5cvo1evXnjmmWcQFxeH8ePHY9iwYdhuIwvEaGdNcdwNERFR6amEEELpIgBApVLht99+Q58+fYo85/3338fmzZtx6tQp3bH+/fsjJSUF24oYrJKdnY3s7Gzd7bS0NAQEBCA1NRXu7u5Gq98Y9uwBnn0WqFIFSEoC7NhpSEREBEC+f3t4eJTo/duq3j6joqIQpp1W9D/dunVDVFRUkfeZNWsWPDw8dJeAgABTl1lqbdsCrq7ArVvA8eNKV0NERGSdrCrcJCUlwcfHR++Yj48P0tLScP/+/ULvM3nyZKSmpuouiYmJ5ii1VBwdgc6d5XV2TREREZWOVYWb0nBycoK7u7vexZJxSjgREdGTsapw4+vri+RHts5OTk6Gu7s7XFxcFKrKuLSDiqOigHv3lK2FiIjIGllVuAkNDUVkZKTesZ07dyI0NFShioyvRg2gQQNAowF27VK6GiIiIuujaLjJyMhAXFwc4uLiAMip3nFxcUhISAAgx8uEh4frzh85ciQuXbqE9957D2fOnMH333+PNWvW4J133lGifJPhlHAiIqLSUzTcHDt2DMHBwQgODgYAREREIDg4GNOmTQMA3LhxQxd0ACAwMBCbN2/Gzp07ERQUhNmzZ+PHH39EN+3yvjYi/7gby5ioT0REZD0sZp0bczFknrxSHjwAPD2BrCwgLg4IClK6IiIiImXZ7Do3ZYWzM/DMM/I6u6aIiIgMw3BjoTglnIiIqHQYbiyUdlDxoUNAWpqytRAREVkThhsLVasWUKcOkJsLPDL7nYiIiIrBcGPBOCWciIjIcAw3FoxTwomIiAzHcGPBOnYEnJyAxETg9GmlqyEiIrIODDcWrHx5oFMneZ2zpoiIiEqG4cbCcdwNERGRYRhuLJx23M2BA0BGhrK1EBERWQOGGwtXty4QGAjk5AB79ihdDRERkeVjuLFwKhW7poiIiAzBcGMFtF1TW7dySjgREdHjMNxYgWeeARwdgStXgHPnlK6GiIjIsjHcWAFXV6B9e3mdU8KJiIiKx3BjJTjuhoiIqGQYbqyEdtzNvn3A/fvK1kJERGTJGG6sRMOGQLVqwIMHwN69SldDRERkuRhurIRKpT9rioiIiArHcGNF8u8STkRERIVjuLEinTsD5coB588DFy8qXQ0REZFlYrixIu7uQNu28jpbb4iIiArHcGNlOCWciIioeAw3VkY77mbPHjlzioiIiPQx3FiZpk0BPz8gKws4cEDpaoiIiCwPw42Vyb9LOMfdEBERFcRwY4U47oaIiKhoDDdWqEsXwM4OiI8Hrl5VuhoiIiLLwnBjhSpVAkJD5XV2TREREeljuLFS7JoiIiIqHMONldJOCY+MBHJylK2FiIjIkjDcWKngYMDbG8jIAA4dUroaIiIiy8FwY6Xs7IBu3eR1jrshIiLKw3BjxTjuhoiIqCCGGyvWtatc1O/kSeDaNaWrISIisgwMN1bMywto1UpeZ9cUERGRxHBj5bSzphhuiIiIJIsIN/PmzUPNmjXh7OyMkJAQREdHF3nuw4cP8fHHH6NWrVpwdnZGUFAQtpXhd3ZtuNm5E8jNVbYWIiIiS6B4uFm9ejUiIiIwffp0xMbGIigoCN26dcPNmzcLPX/KlCn44Ycf8O233+L06dMYOXIkXnjhBRw/ftzMlVuGli0BT08gNRWIilK6GiIiIuUpHm6++uorDB8+HIMHD0bDhg2xYMEClC9fHkuWLCn0/J9//hkffPABevbsiaeeegqjRo1Cz549MXv27ELPz87ORlpamt7Fltjby4HFALumiIiIAIXDTU5ODmJiYhAWFqY7Zmdnh7CwMEQV0QyRnZ0NZ2dnvWMuLi44ePBgoefPmjULHh4euktAQIDxXoCF4JRwIiKiPIqGm9u3b0OtVsPHx0fvuI+PD5KSkgq9T7du3fDVV1/h/Pnz0Gg02LlzJzZs2IAbN24Uev7kyZORmpqquyQmJhr9dShNu5jf8eNAEd82IiKiMkPxbilDff3116hTpw7q168PR0dHjBkzBoMHD4adXeEvxcnJCe7u7noXW+PjAzRvLq9v365sLUREREpTNNx4eXnB3t4eycnJeseTk5Ph6+tb6H2qVKmC33//HZmZmbh69SrOnDkDV1dXPPXUU+Yo2WJxSjgREZGkaLhxdHREixYtEBkZqTum0WgQGRmJ0NDQYu/r7OwMf39/5ObmYv369fjPf/5j6nItmnbczY4dgFqtbC1ERERKUrxbKiIiAosWLcLy5csRHx+PUaNGITMzE4MHDwYAhIeHY/Lkybrzjxw5gg0bNuDSpUs4cOAAunfvDo1Gg/fee0+pl2ARnn4aqFgRuHsX2LNH6WqIiIiUU07pAvr164dbt25h2rRpSEpKQrNmzbBt2zbdIOOEhAS98TQPHjzAlClTcOnSJbi6uqJnz574+eefUbFiRYVegWUoVw4YMACYNw/47jsg3wQ0IiKiMkUlhBCG3unAgQP44YcfcPHiRaxbtw7+/v74+eefERgYiHbt2pmiTqNJS0uDh4cHUlNTbW5w8ZkzQIMGgJ0dcPEiULOm0hUREREZhyHv3wZ3S61fvx7dunWDi4sLjh8/juzsbABAamoqZs6cWbqKySjq1we6dAE0GuD775WuhoiISBkGh5sZM2ZgwYIFWLRoERwcHHTH27Zti9jYWKMWR4YbO1b+++OPQFaWsrUQEREpweBwc/bsWXTo0KHAcQ8PD6SkpBijJnoCPXsCgYHAvXvAihVKV0NERGR+BocbX19fXLhwocDxgwcPlvm1ZiyBvT0werS8/u23gOEjqoiIiKybweFm+PDhGDduHI4cOQKVSoXr169jxYoVePfddzFq1ChT1EgGGjIEKF8eOHkS2L9f6WqIiIjMy+Cp4JMmTYJGo0Hnzp2RlZWFDh06wMnJCe+++y7Gagd8kKIqVQJefx1YuFC23nTsqHRFRERE5mPQVHC1Wo1Dhw6hadOmKF++PC5cuICMjAw0bNgQrq6upqzTaGx5Knh+p04BTZrIbqrLlwEb3AydiIjKEJNNBbe3t0fXrl1x7949ODo6omHDhmjdurXVBJuypHFj4Jln5FYM8+crXQ0REZH5GDzmpnHjxrh06ZIpaiEj0/YSLlwI3L+vbC1ERETmUqp1bt599138+eefuHHjBtLS0vQuZDl69waqVwfu3AFWrVK6GiIiIvMwePuF/Ps8qVQq3XUhBFQqFdQWviV1WRlzo/XZZ8CkSUBwMBATA+T7kREREVkNQ96/DZ4ttYdbTluVYcOA//4XOH4c+OsvoG1bpSsiIiIyLYPDTUfOK7Yqnp7Aa68BS5bIaeEMN0REZOtKtSt4SkoKFi9ejPj4eABAo0aNMGTIEHh4eBi9QGMra91SABAXJ7ulypUDrl4FqlZVuiIiIiLDmHRX8GPHjqFWrVqYM2cO7t69i7t37+Krr75CrVq1uHGmhWrWDGjfHsjNBRYsULoaIiIi0zK45aZ9+/aoXbs2Fi1ahHLlZK9Wbm4uhg0bhkuXLmG/ha/3XxZbbgBg7Vqgb1/A2xtISACcnJSuiIiIqOQMef82ONy4uLjg+PHjqF+/vt7x06dPo2XLlsjKyjK8YjMqq+Hm4UO5W/i1a8DPP8vtGYiIiKyFSbul3N3dkZCQUOB4YmIi3NzcDH04MhMHB0C7r+m33ypbCxERkSkZHG769euHoUOHYvXq1UhMTERiYiJWrVqFYcOG4dVXXzVFjWQkw4cDjo5AdDRw5IjS1RAREZmGwVPBv/zyS6hUKoSHhyM3NxcA4ODggFGjRuH//u//jF4gGY+3N9C/P/DTT7L1JiRE6YqIiIiMr1RTwQEgKysLFy9eBADUqlUL5cuXN2phplJWx9xoHTsGtGolu6kSEgBfX6UrIiIiejyTjrlJTU3F3bt3Ub58eTRp0gRNmjRB+fLlcffuXe4tZQVatgRCQ+UA44ULla6GiIjI+AwON/3798eqQnZhXLNmDfr372+Uosi0tLuFL1gA5OQoWwsREZGxGRxujhw5gmeeeabA8U6dOuEIR6lahZdekt1RN24AGzYoXQ0REZFxGRxusrOzdQOJ83v48CHu379vlKLItBwdgZEj5XVOCyciIltjcLhp3bo1FhYyWGPBggVo0aKFUYoi03vzTTmo+K+/gJgYpashIiIyHoOngs+YMQNhYWE4ceIEOnfuDACIjIzE0aNHsWPHDqMXSKbh6wu88gqwcqVsvVm2TOmKiIiIjMPglpu2bdsiKioKAQEBWLNmDf744w/Url0bf//9N9q3b2+KGslEtAOLV60Cbt1SthYiIiJjKfU6N9aqrK9zk58QQOvWcu2bTz8FPvhA6YqIiIgKZ9J1bmJjY3Hy5End7Y0bN6JPnz744IMPkMN5xVZFpQLefltenz8fKGScOBERkdUxONy8+eabOHfuHADg0qVL6NevH8qXL4+1a9fivffeM3qBZFp9+8ptGf79F/j9d6WrISIienIGh5tz586hWbNmAIC1a9eiY8eOWLlyJZYtW4b169cbuz4yMScnYMQIeZ3TwomIyBYYHG6EENBoNACAXbt2oWfPngCAgIAA3L5927jVkVmMHAmUKwfs3w/8/bfS1RARET0Zg8NNy5YtMWPGDPz888/Yt28fevXqBQC4fPkyfHx8jF4gmZ6/P/Dii/I6W2+IiMjaGRxu5s6di9jYWIwZMwYffvghateuDQBYt24d2rRpY/QCyTy008J/+QW4c0fZWoiIiJ6E0aaCP3jwAPb29nBwcDDGw5kMp4IXTgigeXMgLg747DOAY8OJiMiSmHQqeFGcnZ0tPthQ0VSqvNab778H1Gpl6yEiIioto4WbJzFv3jzUrFkTzs7OCAkJQXR0dLHnz507F/Xq1YOLiwsCAgLwzjvv4MGDB2aq1na9+irg6QlcvQr88YfS1RAREZWO4uFm9erViIiIwPTp0xEbG4ugoCB069YNN2/eLPT8lStXYtKkSZg+fTri4+OxePFirF69Gh9wed0n5uICDB8ur3NgMRERWSvFt18ICQlBq1at8N133wEANBoNAgICMHbsWEyaNKnA+WPGjEF8fDwiIyN1xyZMmIAjR47g4MGDBc7Pzs5Gdna27nZaWhoCAgI45qYICQlAYCCg0QCnTgGNGildERERkUJjbkojJycHMTExCAsL0x2zs7NDWFgYoqKiCr1PmzZtEBMTo+u6unTpErZs2aJbb+dRs2bNgoeHh+4SEBBg/BdiQ6pXB/r0kdf/lzeJiIisSjlD76BWq7Fs2TJERkbi5s2bugX9tHbv3l3ix7p9+zbUanWB9XF8fHxw5syZQu/z2muv4fbt22jXrh2EEMjNzcXIkSOL7JaaPHkyIiIidLe1LTdUtLFjgQ0bgJ9+AmbNAipWVLoiIiKikjO45WbcuHEYN24c1Go1GjdujKCgIL2Lqe3duxczZ87E999/j9jYWGzYsAGbN2/GJ598Uuj5Tk5OcHd317tQ8Tp2BBo3BrKygCVLlK6GiIjIMAa33KxatQpr1qwpshvIEF5eXrC3t0dycrLe8eTkZPj6+hZ6n6lTp2LgwIEYNmwYAKBJkybIzMzEiBEj8OGHH8LOTvEx0lZPOy38zTeBefOAceMAe3ulqyIiIioZg5OAo6OjblXiJ+Xo6IgWLVroDQ7WaDSIjIxEaGhooffJysoqEGDs//fOq/DYaJsyYIDsjrp0Cdi6VelqiIiISs7gcDNhwgR8/fXXRgsSERERWLRoEZYvX474+HiMGjUKmZmZGDx4MAAgPDwckydP1p3fu3dvzJ8/H6tWrcLly5exc+dOTJ06Fb1799aFHHpyFSoA/2sc47RwIiKyKgZ3Sx08eBB79uzB1q1b0ahRowKrEm/YsMGgx+vXrx9u3bqFadOmISkpCc2aNcO2bdt0g4wTEhL0WmqmTJkClUqFKVOm4Nq1a6hSpQp69+6NTz/91NCXQo/x1lvA7NnAjh3A2bNAvXpKV0RERPR4Bq9zo21RKcrSpUufqCBT495ShvnPf4BNm4AxY9iCQ0REyjHk/VvxRfzMjeHGMLt2AV26AK6uwLVrAL9lRESkBLMs4nfr1i0cPHgQBw8exK1bt0r7MGThOncGGjQAMjKA5cuVroaIiOjxDA43mZmZGDJkCPz8/NChQwd06NABVatWxdChQ5GVlWWKGklBKpXskgJkt9QjazYSERFZHIPDTUREBPbt24c//vgDKSkpSElJwcaNG7Fv3z5MmDDBFDWSwsLDZXfU+fNycDEREZElMzjcrF+/HosXL0aPHj10K/727NkTixYtwrp160xRIynM1RXQjiPnoGIiIrJ0BoebrKysAntBAYC3tze7pWzYmDGyi2rrVuDCBaWrISIiKprB4SY0NBTTp0/HgwcPdMfu37+Pjz76qMhVhcn61a4N9OgBCCG3ZCAiIrJUBk8FP3XqFLp164bs7GzdRpknTpyAs7Mztm/fjkaNGpmkUGPhVPDS27ZNBhx3dzkt3NVV6YqIiKisMPk6N1lZWVixYgXOnDkDAGjQoAEGDBgAFxeX0lVsRgw3pafRAPXry4HF338PjBqldEVERFRWcBG/YjDcPJlvvpG7hDdsCJw6JcfhEBERmZrRw82mTZvQo0cPODg4YNOmTcWe+/zzzxtWrZkx3DyZtDTA318u6rdrl1zkj4iIyNSMHm7s7OyQlJQEb29vvU0sCzyYSgW1Wm14xWbEcPPkxoyRg4qffx7YuFHpaoiIqCww+vYLGo0G3t7euutFXSw92JBxaFcs/uMP4PJlZWshIiJ6lMFTwX/66SdkZ2cXOJ6Tk4OffvrJKEWRZatfH+jaVU4L//57pashIiLSZ/CAYnt7e9y4cUPXkqN1584deHt7W3zrDbuljOPPP4HevYFKlYB//wXKl1e6IiIismUm3RVcCAFVIVNk/v33X3h4eBj6cGSlevQAnnoKuHcPWLFC6WqIiIjylCvpicHBwVCpVFCpVOjcuTPKlcu7q1qtxuXLl9G9e3eTFEmWx94eGD0amDBB7jc1bBinhRMRkWUocbjp06cPACAuLg7dunWDa77laR0dHVGzZk289NJLRi+QLNeQIcDUqcDJk8D+/UDHjkpXREREZEC4mT59OgCgZs2a6NevH5ydnU1WFFmHihWBgQOBH36QrTdKh5u0NOD2bSAwkK1IRERlGVcopidy6hTQpAlgZyenhVevbvrnvHcPOH264OXff+XXe/QA1qzh3ldERLbEkPfvErfcaKnVasyZMwdr1qxBQkICcnJy9L5+9+5dQx+SrFjjxsAzzwB79gDz5wOzZhnvsW/dKjzEJCUVfR+VCti6FXj2WWDzZqBKFePVQ0RE1sHgcPPRRx/hxx9/xIQJEzBlyhR8+OGHuHLlCn7//XdMmzbNFDWShXv7bRluFi0Cpk0DDNk/VQgZVgoLMbdvF32/gAC5v1X+S4MGwLlzQK9ewNGjQNu2wI4dQM2aT/wSiYjIihjcLVWrVi1888036NWrF9zc3BAXF6c7dvjwYaxcudJUtRoFu6WMT60GatUCrl4FliwBBg8ueI4QstuosBCTklL0YwcGFgwx9esDxf3ozp6ViwwmJAC+vsC2bUBQ0BO/TCIiUpBJu6WSkpLQpEkTAICrqytSU1MBAM899xymTp1ainLJ2tnbA2+9Bbz/vhxY3KlTwQATHw+kpxd+fzs7GY4eDTH16gEVKhheT716QFQU0L27nMnVoYPcA6tTpyd5lUREZC0MDjfVqlXDjRs3UL16ddSqVQs7duxA8+bNcfToUTg5OZmiRrICQ4cC06cDx4/Lxf0KU64cUKdOwRBTty5g7Ml3VavK6en/+Y/8t1s3udjgyy8b93mIiMjyGBxuXnjhBURGRiIkJARjx47F66+/jsWLFyMhIQHvvPOOKWokK+DpCYwaBcyZAzg4yNaTR0NMnTqAo6P5aqpYEdi+HRgwANiwAejbF/juO9nKREREtuuJp4JHRUUhKioKderUQe/evY1Vl8lwzI3paDTAjRuAj49spbEUarXcyXzBAnl7yhTg44+5Fg4RkTUx5P2b69xQmSAE8MknsusMkNtFzJ9vWSGMiIiKZvQBxZs2bSrxkz///PMlPpfIXFQqOU3dx0d2S/34I3DzJrBqlWFT14mIyPKVqOXGzk5/83CVSoVH76bdKVytVhuxPONjyw39/jvQvz+QnS3Xwtm0CahcWemqiIioOIa8f9sV+9X/0Wg0usuOHTvQrFkzbN26FSkpKUhJScHWrVvRvHlzbNu2zSgvgMiU+vQBdu6UA44PHQLat8/buoGIiKyfwWNuGjdujAULFqBdu3Z6xw8cOIARI0YgPj7eqAUaG1tuSOvUKTlF/Pp1ueLxtm1yVhcREVkeo7fc5Hfx4kVUrFixwHEPDw9cuXLF0IcjUkzjxsBff8kVjxMTgXbt5G0iIrJuBoebVq1aISIiAsnJybpjycnJmDhxIlq3bm3U4ohMrUYN4OBB4Omn5W7jYWHAH38oXRURET0Jg8PNkiVLdCsU165dG7Vr10b16tVx7do1LF682BQ1EpmUpyewa5fccPP+feCFF+QeWUREZJ1Ktc6NEAI7d+7EmTNnAAANGjRAWFiYbsaUJeOYGyrKw4fAiBHAsmXy9owZwAcfcLE/IiJLwEX8isFwQ8URAvjwQ2DWLHl7zBhg7ly5OSgRESnH6Iv4ffPNNxgxYgScnZ3xzTffFHvu22+/XfJK/2fevHn44osvkJSUhKCgIHz77bdFjt/p1KkT9u3bV+B4z549sXnzZoOfmyg/lQqYORPw9QXGj5d7USUnAz//DHBfWCIi61CilpvAwEAcO3YMnp6eCAwMLPrBVCpcunTJoAJWr16N8PBwLFiwACEhIZg7dy7Wrl2Ls2fPwtvbu8D5d+/eRU5Oju72nTt3EBQUhB9//BFvvPHGY5+PLTdUUqtXAwMHyu6qZ54BfvsN8PBQuioiorLJqrqlQkJC0KpVK3z33XcA5IKBAQEBGDt2LCZNmvTY+8+dOxfTpk3DjRs3UKFChQJfz87ORnZ2tu52WloaAgICGG6oRCIj5QDj9HQgKAjYuhXw81O6KiKissek69wYU05ODmJiYhAWFqY7Zmdnh7CwMERFRZXoMRYvXoz+/fsXGmwAYNasWfDw8NBdAgICjFI7lQ2dOwP79sk9qU6cANq0Ac6fV7oqIiIqTonG3ERERJT4Ab/66qsSn3v79m2o1Wr4+PjoHffx8dHNxCpOdHQ0Tp06VewU9MmTJ+vVr225ISqp4GC5uF/XrsDFizLgbNkCtGqldGVERFSYEoWb48ePl+jBzD0VfPHixWjSpEmxiwc6OTnBiSNB6Qk99ZQMOD17AjExcgzO+vVy+wYiIrIsJQo3e/bsMcmTe3l5wd7eXm+1Y0CueOzr61vsfTMzM7Fq1Sp8/PHHJqmN6FHe3sCePcBLL8mNN597Tq6JM2CA0pUREVF+io65cXR0RIsWLRAZGak7ptFoEBkZidDQ0GLvu3btWmRnZ+P11183dZlEOm5uwJ9/Aq++CuTmAq+/DsyerXRVRESUX4labh517NgxrFmzBgkJCXrTsgFgw4YNBj1WREQEBg0ahJYtW6J169aYO3cuMjMzMXjwYABAeHg4/P39MUu7qtr/LF68GH369IGnp2dpXgJRqTk6Ar/8ItfCmTMHePddICkJ+OwzwE7RjwtERASUouVm1apVaNOmDeLj4/Hbb7/h4cOH+Oeff7B79254lGIRkH79+uHLL7/EtGnT0KxZM8TFxWHbtm26QcYJCQm4ceOG3n3Onj2LgwcPYujQoQY/H5Ex2NnJFpvPP5e3v/wSGDRIrolDRETKMnidm6ZNm+LNN9/E6NGj4ebmhhMnTiAwMBBvvvkm/Pz88NFHH5mqVqPgIn5kbD/9BAwZAqjVcoDxunWAq6vSVRER2RaTrnNz8eJF9OrVC4AcM5OZmQmVSoV33nkHCxcuLF3FRFYsPBz44w+gfHlg+3Y58DgsDPj4Y7lGzoMHSldIRFS2GBxuKlWqhPT0dACAv78/Tp06BQBISUlBVlaWcasjshI9egC7dwPVqwP378uVjadPBzp1kls2dOgATJkiZ1llZipdLRGZyv79cqLBkSNKV1K2GRxuOnTogJ07dwIAXnnlFYwbNw7Dhw/Hq6++is6dOxu9QCJrERICXL4MnDoFfP890K+fHHSckwMcOAB8+qlcCLBiReDpp4H33wc2bwZSU5WunIiMYfNm+X98xQqgfXvg228BZTc4KrtKPObm1KlTaNy4Me7evYsHDx6gatWq0Gg0+Pzzz/HXX3+hTp06mDJlCipVqmTqmp8Ix9yQOQkBXLggP83t2ycvCQn659jZyX2rOnaULTwdOgCcBEhkXX77TX6gefgQqFYN+Pdfebx/f2DhQrmMBD0Zk2ycaWdnh1atWmHYsGHo378/3Kz0J8VwQ0q7elWGHG3guXCh4DmNG8uQow08j1nTkogU9OuvwMCBclJBv37Azz8D8+YBEyfK9bDq15cTDRo1UrpS62aScHPgwAEsXboU69atg0ajwUsvvYRhw4ahffv2RinaXBhuyNJcvy6DjjbsnD5d8Jy6dfPCTseOALdHI7IMS5cCQ4fKVtpBg4DFiwF7e/m1v/4C+vYFrl2TEw4WLQJee03Zeq2ZScKNVmZmJtasWYNly5bhwIEDqF27NoYOHYpBgwY9dssES8BwQ5bu1i05RkfbunPiRMF++5o181p1OnaUe1+ZeWs3ojJv/nzgrbfk9TfflGPtHl3I8+ZNuUXLrl3y9qhRcvFPbnloOJOGm/wuXLiApUuX4ueff0ZSUhK6d++OTZs2lfbhzILhhqzNvXvAoUN5Y3ZiY2Xzd37+/nnjdVq3BmrVkrO0iMg05swBIiLk9XHj5O2iPmCo1XJpiE8+kR9UWrYE1q6VH1Ko5MwWbgDZkrNixQpMnjwZKSkpUD/6V9fCMNyQtUtPl83d2m6s6OjCV0b28pIhp3Zt+a/2Uru2XIvHmlt6NBrgxg3gypWCl/R04OWXgWHD5Mw0ImObORP48EN5ffJkOROyJP+ftm2TrTh37wKVKsmxOf9bNo5KwCzhZv/+/ViyZAnWr18POzs79O3bF0OHDsXTTz9dqqLNheGGbM39+8Dhw3lh559/ZFN4cSpUKDr4BATkjRlQSnHh5coVOePskW3tCnB1BQYPlp+qa9UydcVUFggBTJsGzJghb3/8sVy/ypAPCgkJwCuvyA8lgAxJH32k/P85a2CycHP9+nUsW7YMy5Ytw4ULF9CmTRsMHToUffv2RYUKFZ64cHNguKGyID0duHgx73LhQt71hITi195wcJDN5YUFn8BAwNn5yetTqwsPL1ev5v37uH267O3look1a+pfsrKA776TIQ+QbzzPPw+8847strPmFitSjhBy9tPs2fL255/L26WRnQ1MmCBnVAHAs88CK1cC/9tSkYpgknDTo0cP7Nq1C15eXggPD8eQIUNQr149oxRsTgw3VNZlZ8sAUVjwuXSp+BYRlUqO78kffPJf147zKSq85G95eVx4KVdOtiI9Gl60l6pV5TmFEUIO4JwzB9i6Ne94cDAwfrxce8TR8XHfKSJJowHGjpUDhgG5ON+YMU/+uL/+CgwfLlctr1oVWL0aaNfuyR/XVpkk3Dz//PMYOnQonnvuOdhbcfsZww1R0dRqOW310dCjvf2/nVeK5OkJuLvLBcxKEl4ebXmpUaNk4cUQ8fHAN98Ay5fLLjxArhs0ejQwcqQcm0RUFLUaGDECWLJEhvuFC+V4LmOJjwdeekn+a28PfPaZHKjMFsaCzDqg2Now3BCVjhDA7dsFg4/2+qPjfAoLL4+2vJjzc9KdO/KN6bvv5NpCgOxie/112ZrDBdboUbm5cu2alSvlFO/ly+Xvi7FlZMip5CtXytsvvCDXz+GMR30MN8VguCEyjfR02a2Vni5bYMwdXkoqJ0dOw50zB4iJyTvetascl9OtGz81k/w9ee01YP16GdRXrpQDgU1FCGDBAhm0c3Jkd++6dXJrFpIYborBcENEgHwzOXRIhpzff5fjKgCgQQM5w2rgQLmqLJU9Dx7IIPPnn3Js1rp1QO/e5nnuo0flUgYJCbJl8fvv5aw/Muz92+BdwYmIbIFKJQdvrl8vu9beeUdubhgfL8fiVK8up+lqu7CobMjKkrPr/vxThotNm8wXbACgVSu5UGePHjJkDRkix/hox4tRyTDcEFGZFxgIfPWVHAg9Z468feeOXKytZk3ZipO/C4tsU3q6DBU7d8q1oLZuld2U5ubpKcPVjBlyrM/ixUBoaOGb7FLhGG6IiP7H3V2OeTh/HtiwAWjfXs76+uUXuWR+hw7Ab78V3P6CrF9Kihx3tX+//D3YsQPo1Em5euzsZMvhjh1AlSpyj7kWLWQXKj0eww0R0SPs7eWMlf37gWPH5AyZcuXkhqYvvih3af/6ayAtTelKyRju3AE6d5YrfVeqBERGAm3aKF2V1LkzcPw40Lat/H174QW5eODjlloo6xhuiIiK0aKF3APo6lXggw+AypXlrLDx4+UigxERcmFCsk7JybKFJjZWtpDs3Stb6SyJvz+wZ0/eRp1ffilDD8eDFY3hhoioBKpWlRskJibKKbv168tP0nPmyNWZX34ZOHiw+K0tyLJcuwZ07AicOgX4+cm92Zo2Vbqqwjk4yK0f1q+XA98PHJArbu/Zo3RllolTwYmISkGjkeMh5syR/2q1aCHfIJ2dAReXvH/zXzfka8ZYpZkKunpV7ul06ZKcGRcZKdeWsQbnz8sw/fffcmzOjBnA++/L67aM69wUg+GGiIztn3+AuXNl91V2tnEfu1y5ooNPUeEoMBBo3lxeKlY0bj224MIFGWwSE4GnngJ275YLT1qTrCy5hciyZfJ2r17ATz/JblNbxXBTDIYbIjKVW7fkbKq7d+UaJffvy4v2+qP/FnWsuM1LDfXUU7I1qUWLvMDj6Wm8x7c28fFyvMqNG0C9erLFxt9f6apKb/FiGXKys+WyBWvXWt6YIWNhuCkGww0RWTqNRoadx4WgwkJTZiZw5oxcl6eogc41augHnhYt5GBaW3fiBNCliwyhTZrI9Wx8fJSu6skdPy67qS5dkisqf/ON3OzT1rYRYbgpBsMNEZUVd+7IN76YGDkbKCZGbnJamGrVCgYeX1/z1mtKx47JdWzu3ZOvb8cO22rBSkkB3ngD2LhR3vbxkS1T+S9168ouSwcHJSstPYabYjDcEFFZlpJSMPCcO1f4uX5+BQNP1arW1yLw119y5eG0NODpp+XKw7Y4FkkIOU18ypSiuzbLlZOz++rWLRh8vL0t+2fLcFMMhhsiIn1paUBcnH7gOXOm8GntPj55QUcbegICLPdNce9e4LnnZHddhw5yWwM3N6WrMq30dPnzO3tWBtezZ/OuF7dHlYdHXtDJH3rq1LGMTWQZborBcENE9HgZGXKMSv7Ac/p03u7p+Xl55QWe5s3lGkBVqshuHyWnsm/fDvTpI8cidekity6whDdppWg0cm2fwkLPlSvFr9FUvXrhwad6dfNNQWe4KQbDDRFR6WRlybVVtGEnJkZOg8/NLfo+lSrJ8OPlJQNPYdfz33Z3N04r0KZNwCuvyO6Z556Ts4icnZ/8cW3VgwdyivyjwefsWTlOqSjOzrJlp7BurkqVjFsjw00xGG6IiIznwQPg5En9wHPlipwOXxrlyhUffgr72qOhZe1a4LXXZOh66SVg5Uo5i4hK5/btwkPPhQtF73Hl4iJb/4zZqsNwUwyGGyIi08vNlZ/4b92Sb47aS3G3MzNL91yurnlBR7vxpUYDDBggF7njKs+mkZsrV3ourJvL01N2axoTw00xGG6IiCzT/fv6wedxYej27aK7xIYOBX74Qe7wTuaXk2P81jJD3r+ZZ4mIyCK4uMiZVwEBJTtfCDnT69HA4+kptyOw9b2WLJnS3YAMN0REZJVUKjl92cPDeja9JPNgriUiIiKbYhHhZt68eahZsyacnZ0REhKC6OjoYs9PSUnB6NGj4efnBycnJ9StWxdbtmwxU7VERERkyRTvllq9ejUiIiKwYMEChISEYO7cuejWrRvOnj0Lb2/vAufn5OSgS5cu8Pb2xrp16+Dv74+rV6+ioi2upU1EREQGU3y2VEhICFq1aoXvvvsOAKDRaBAQEICxY8di0qRJBc5fsGABvvjiC5w5cwYOJdj9Kzs7G9nZ2brbaWlpCAgI4GwpIiIiK2LIbClFu6VycnIQExODsLAw3TE7OzuEhYUhKiqq0Pts2rQJoaGhGD16NHx8fNC4cWPMnDkTarW60PNnzZoFDw8P3SWgpMPwiYiIyCopGm5u374NtVoNHx8fveM+Pj5ISkoq9D6XLl3CunXroFarsWXLFkydOhWzZ8/GjBkzCj1/8uTJSE1N1V0SExON/jqIiIjIcig+5sZQGo0G3t7eWLhwIezt7dGiRQtcu3YNX3zxBaZPn17gfCcnJzg5OSlQKRERESlB0XDj5eUFe3t7JCcn6x1PTk6Gr69voffx8/ODg4MD7PMtO9mgQQMkJSUhJycHjkqvHERERESKUrRbytHRES1atEBkZKTumEajQWRkJEJDQwu9T9u2bXHhwgVoNBrdsXPnzsHPz4/BhoiIiJRf5yYiIgKLFi3C8uXLER8fj1GjRiEzMxODBw8GAISHh2Py5Mm680eNGoW7d+9i3LhxOHfuHDZv3oyZM2di9OjRSr0EIiIisiCKj7np168fbt26hWnTpiEpKQnNmjXDtm3bdIOMExISYJdvg5CAgABs374d77zzDpo2bQp/f3+MGzcO77//vlIvgYiIiCyI4uvcmBt3BSciIrI+VrPODREREZGxMdwQERGRTWG4ISIiIpvCcENEREQ2heGGiIiIbArDDREREdkUhhsiIiKyKQw3REREZFMYboiIiMimMNwQERGRTWG4ISIiIpvCcENEREQ2heGGiIiIbArDDREREdkUhhsiIiKyKQw3REREZFMYboiIiMimMNwQERGRTWG4ISIiIpvCcENEREQ2heGGiIiIbArDDREREdkUhhsiIiKyKQw3REREZFMYboiIiMimMNwQERGRTWG4ISIiIpvCcENEREQ2heGGiIiIbArDDREREdkUhhsiIiKyKQw3REREZFMYboiIiMimlFO6ACIisgG3bgF//QUcPgz4+gIjRwJOTkpXRUoQAlCpFC2B4YaIiAyj0QBnzgCHDslAc+gQcP68/jmLFwPLlwPBwcrUSMpQq4FXXgGefx544w3FymC4ISKi4mVlAdHReUEmKgq4d6/geQ0bAq1bA5s3AydPyutTpwKTJwMODuavm8xv2jTgt9+AbduALl0Af39FymC4ISIifdeu5QWZQ4eAuDggN1f/HBcXICQEaNMGaNsWCA0FKlWSX7t1S3ZLbdgATJ8ObNokW3EaNTL7SyEzWrsWmDlTXl+8WLFgA1jIgOJ58+ahZs2acHZ2RkhICKKjo4s8d9myZVCpVHoXZ2dnM1ZLRGRDcnOB48eBefOA114DatYEqlUD+vYFvv4aOHZMnlO1quxumDsXOHoUSE0F9uwBPv0U6NkzL9gAQJUqwLp1wIoV8nhMDNC8OfDFF7LbgmzPyZN53VDvvgu8+qqi5SjecrN69WpERERgwYIFCAkJwdy5c9GtWzecPXsW3t7ehd7H3d0dZ8+e1d1WKTxwiYjIaqSmAkeO5I2XOXwYyMjQP8fODggKymuVadMGqF7dsEGiKpUMS506AcOHA1u2AO+9B/z+O7BsGVCnjhFfFCnq7l2gTx/ZfRkWBsyapXRFUAkhhJIFhISEoFWrVvjuu+8AABqNBgEBARg7diwmTZpU4Pxly5Zh/PjxSElJKdHjZ2dnIzs7W3c7LS0NAQEBSE1Nhbu7u1FeAxGRRRICuHJFf+DvyZPyeH5ubrJbSRtkQkLkMWPWsXQpMH48kJ4uu7Q++wwYPVoGKbJearVsuduxAwgMlK16np4meaq0tDR4eHiU6P1b0ZabnJwcxMTEYPLkybpjdnZ2CAsLQ1RUVJH3y8jIQI0aNaDRaNC8eXPMnDkTjYroy501axY++ugjo9dORGRxhJDjY/buzRsvk5RU8LzAwLwg07atHAtjb2+6ulQqYMgQoHNnYOhQIDISePttOfB0yRLZFUbW6YMPZLApX162ypko2BhK0Zab69evw9/fH3/99RdCQ0N1x9977z3s27cPR44cKXCfqKgonD9/Hk2bNkVqaiq+/PJL7N+/H//88w+qVatW4Hy23BCRzUtKAn75RbaOnD6t/zUHBzneJX8Xk5+fMnUCchr5ggXAxImyG8PVFfjqK2DYMMXXRiEDrVqVN7Zm9Wo5TsuErKblpjRCQ0P1glCbNm3QoEED/PDDD/jkk08KnO/k5AQnLiRFRLYmJwf4808ZaLZuzRuo6+wsxz20bSsvLVvKbiBLYWcHvPUW0LWrHIB66BAwYoScWfXjj4rOsCEDxMXJ1jgAmDTJ5MHGUIqGGy8vL9jb2yM5OVnveHJyMnx9fUv0GA4ODggODsaFCxdMUSIRkWU5cUIGmhUrgNu3846Hhsqw0K8f4OGhWHklVrs2sG+fnH314YdyXZTGjYFvvwUGDGArjiW7fRt44QXg/n2ge3dgxgylKypA0ZFcjo6OaNGiBSIjI3XHNBoNIiMj9VpniqNWq3Hy5En4KdnMqnX6tJyJQERkTLdvA998I1f7bdZMTtG+fVt2L73/PhAfLwcMjxhhHcFGy94emDBBTkVv1QpISQEGDgRefBF45EMvWYjcXBmgr1wBatUCVq407XitUlJ8mHpERAQWLVqE5cuXIz4+HqNGjUJmZiYGDx4MAAgPD9cbcPzxxx9jx44duHTpEmJjY/H666/j6tWrGDZsmFIvQbp+Xa7GGBoKXLyobC1EZP1yc+VKvy+/LNeYGTdOdgU4OspjmzcDCQnA//0fUL++0tU+mQYNZDibMUOOEfr9d9mKs26d0pXRo95/H9i9G6hQAdi4UX99Iwui+Jibfv364datW5g2bRqSkpLQrFkzbNu2DT4+PgCAhIQE2OWbKnjv3j0MHz4cSUlJqFSpElq0aIG//voLDRs2VOolSLdvy2bU+Hi55PiGDUDHjsrWRETWJz5edjv9/LP+TKfmzYHBg+UATguZkWJU5crJ7qlevYBBg4C//5aLBvbvD3z3nW2+Zmvzyy9y8DcA/PSTRa84rfg6N+ZmyGhrg12/LhcyOnpU/kf9/nu5eBURUXFSUuRsk6VL5QJ7Wl5ewOuvy1DTtKli5ZldTg7w8ceyVUqtlruML1oEPPec0pWVXbGxcoD6gwfAlClAIRN4TM2Q92+GG2O7f1+OIF+1St4eNw748ksZdoiItNRq2by/dKlc7+XBA3nc3l62XgweLBdHc3RUtk4lRUfLVpwzZ+TtwYOBOXOsa1yRLbh5U866S0yUv5ubNimy+CLDTTFMHm4AuZDWjBlyd1QA6NZNfirjf0giunBBbiK5fLl8s9Bq1Ei+eb/+OvC/bnmC/MA4darsDhECCAiQC/+FhSldWdnw8KEcT7pvH1C3rmxZrFhRkVIYbophlnCjtX69HPl//74c8PfHH3L6IxGVLRkZcsfkpUuBAwfyjlesKPdfGjwYaNGC05+Lc/CgbMW5dEneHjUK+PxzuQggmc7bb8vp+W5uMtg0aKBYKYa8fys+W8qmvfSS/A/p7y+bVUNC5C66RGT7hJCfdgcPlmNGhgyRwUalkq25q1YBN27I3bhbtmSweZx27eQaP2+9JW/Pny8398wfFsm4li2TwQaQg4kVDDaGYrgxtebN5QDj1q3lzqlduwI//KB0VURkKgkJcrBl7dpyR+xly4DMTLkL9qefyq9v2ybXCnF2Vrpa6+LqKsPgzp2ye+rSJTkrdcIE2UJOxnP0KDBypLz+3/8Czz+vaDmGYrgxBz8/uZHdq6/KtStGjpRNfbm5SldGxpKRIWd3+PoCTZrIRdbu3lW6KjKn1FRg7Fi5KeW0afKN19VVbhR58CBw9qzcZLCQPfDIQGFhcnfzIUNkC9lXX8kPktHRSldmG5KT5QrE2dnAf/4jxzxZGY65MSchgJkz5TQ6QLbirF6t2OAsMoKcHGDhQvlJ/eZN/a85OcnF1kaMANq3Z7eDrRJCjqcZP152MwGyxWbwYNk1XaGCktXZvs2b5aabSUlyBs+kSTJcck/B0snJkbu3Hzwox4oeOQJYyCbTHHNjqVQquUjV+vVye/gdO4CnnwbOn1e6MjKURiOXHW/QQH5av3lTdkOsXCnXN2rWTH7qWbFCNps3aADMng3cuqV05WRMFy/K6dr9+slgU6cOsGuXHFsXHs5gYw69egH//CMHZms08gNkixZyXZy0NKWrsz7jx8tg4+4uVyC2kGBjMFHGpKamCgAiNTVV2UJiY4WoVk0IQIhKlYSIjFS2HioZjUaILVuECAqSPztACF9fIebPFyInR/+8o0eFGD5ciAoV8s51cBCiXz8hdu0SQq1W7GXQE3rwQIgZM4RwdpY/V0dHIf77XyHu31e6srJt3TohvLzy/r+5uAgRHi7Enj38/1YSixbJ75tKJcSffypdTQGGvH8z3Cjpxg0hQkLkL5O9vXyDJMsVFSVEx455fzjd3YX49FMhMjKKv19amhALFwrRsmXefQEhatUSYtYs+XtA1mPvXiHq18/7OXbuLMTZs0pXRVq3bgnx+ef6PyNAiKeeEuKTT4RISFC6Qsv0118ypAMyuFsghptiWFS4EUJ+0hswIO8/4JgxQjx8qHRVlF98vBAvvJD3M3JyEuLdd4W4fdvwx4qNFWLUKBmMtI9XrpwQL74oxLZt/HRpyW7eFGLQoLyfm7e3ECtWyFY6sjwajXzDHj5cCDe3vJ+bSiVEt25CrFrFljata9eE8POT35+XXrLY32mGm2JYXLgRQv4izZyZ95+vSxch7t5VuipKTBRi6FAh7Ozkz8XOToghQ4zzyS8jQ4glS4QIDdX/dFmjhvx0+e+/T/4cZBxqtRA//ihE5cp5b44jR/L/qDXJyBBi+XL9llftkIAxY+SHjrLqwYO8v0ONGgmRnq50RUViuCmGRYYbrd9+yxufUbcum7qVcueOEBMn5o2nAIT4z3+E+Ocf0zzfyZNCvP22EBUr5j2fnZ0Qzz8vxB9/CJGba5rnpcc7eVKIdu3yfi5BQbJ7kqzXhQtCTJmSN+ZRe2nWTIhvvildi6w1Gz5cvv6KFYU4f17paorFcFMMiw43QggRFydEQEDeL9vOnUpXVHZkZsoxMB4eeX/w2rcX4tAh8zx/VpYQP/8snzP/H91q1YSYPl2Iq1fNUwfJT/rvvy+7DAH5oWP2bHYZ25LcXNkV3K9f3lgT7eDwV14RYutW2/9gsWBBXmvk1q1KV/NYDDfFsPhwI4QQSUl5zYT29kLMm6d0RbYtJ0eIH34QomrVvD9wTZoIsXmzcn3Pp08LEREhhKen/liBnj1lC1/+mVlkXH/+KbsHtd/3Pn04CNXW3bkjxLffChEcrP/Bwt9fiA8/tPgWjVI5cEDO3gSE+L//U7qaEmG4KYZVhBsh5EC3gQPz/pO99Rbf0IxNoxFi7VrZBaj9PtesKVtPLOUT24MHQvz6qxDPPqv/R9fPT4gPPhDi0iWlK7QdiYlyYLf2e1y9uhAbNypdFZlbbKwQY8fmjbHSXjp0EGLZssfPjrQGiYlC+PjI19W3r8UOIH6UIe/fXKHYkgkBfPaZXLJdCLlq5Nq1QKVKSldm/SIj5Uqmx47J215econxN9+03JVNz58HFi+WO0vnXw25Sxe5CvLzzwOOjqZ5brVa7t1T2OXBA/3b5cvLTQ69vU1Ti7Hl5srNAadNk9to2NsDERHA9OlchK8sy84GNm0CliyRC65qNPK4qyvQv79cgTo01PpWHn/wQC4sGh0NNG0K/PWX1fyeG/L+zXBjDTZuBAYMyNt8748/gHr1lK7KOsXGylCzc6e87eoqN92bMAFwc1O2tpLKyZG/AwsXyj+6WlWqAG+8IZf+zx84Hg0fjwsnhX394UPD62zcGHj2WeCZZ+QfU0sM5dHRMtDGxcnbbdoACxbI/cGItP79F/jpJxl0Ll7MO16vntzfauBAuYegpRNC7nW2dClQubLcHPOpp5SuqsQYbophleEGAP7+G+jdW+4oXLEisGaN/MROJXPhgtzTa/VqedvBQW5gOmWK9bQwFObyZdmas2RJ3r5GpuboCLi45F2cnfVvJyfLTQ3zU6mA4GAZdJ59Vu61pWSYTEmRW6HMny//4FeqBHz+uXyjsuOuNFQEIYADB+T/t7VrgawsedzeXm7DMWSI3A7CwUHZOosybx4wZoz8Hd++XW5AakUYbophteEGkG8aL74omxHt7eXO06NHK12VZUtKkrt1L1okux9UKrkHzccfW9UnlsfKzZUbCC5dCiQm6oeNwgJIaY47O8uLvf3j67l1C9i3D9i9W+6zdOaM/tft7YGWLfNadtq2ld1ZpiaEDLjvvCN/NwC5B9QXX1h3yCXzS0+XHzKXLJF/k7WqVJEt7WFhQEiI7PK2BPv2yZpyc4Evv5St1VaG4aYYVh1uANkPPGKEbCIFgFGjZMix1E8KSklNlW9Yc+bkfbrq0QOYNQsIClK2trLoxg0ZcvbskYHn0iX9rzs4yE1ktS07Tz9t/LFPFy4Ab72V1yVZr55suXnmGeM+D5U9Z84Ay5YBy5fnhWatWrXk7/PTT8uwExRkurFxRUlIkB8mbt2SH+5++cX6xgqB4aZYVh9uAPnp88svgfffl9effVY2kVaurHRlynvwQO7K/emnwN278lhIiByY3bGjsrVRnoSEvKCzZ49sbcrP2VmOf9G27LRqVfoAn50tf/4zZ8rrTk6yS+q99yx38DhZp9xcYNs2YP164PDhgi2WgPyda9FC/l3Shp6AANOFjfv3ZTdwTAzQrBlw6JB5WklNgOGmGDYRbrQ2bZLNnxkZQO3acpBp/fpKV2UeQgC3b8sZROfOyX/Pn5fNw9euyXPq15dvaH36WOWnlDJDCDlIM3/LTnKy/jkVKsg/0NqWneDgknWP7d4tWzfPnZO3u3SR4bd2beO/DqJH3bsnB60fOSLDzuHD8tijfH31W3datpSTHZ6UELLb9ZdfZPfYsWNAjRpP/rgKYbgphk2FG0AO3OzdG7h6FfDwAH78Uf4H8fGxja6qlBT98JI/zKSmFn4ff3/go4+AQYOAcuXMWi4ZgRDyE6+2VWfvXuDOHf1zPDxkS9wzz8hLkyb6A4Fv3pRjCn75Rd729QXmzgX69mXQJeUIIbtHtUHnyBHgxAnZ4pOfnZ38nc7fulOvnuGD3efOlePL7O1ld6yVd8Ey3BTD5sINIP+Qv/iibG7UUqlkUvf1lVMUtZfCbhvjE8KTyMjQDy/5A8zt20XfT6WSzbl16shL3bry8uyzcgAs2QaNRoZ4bavOvn1AWpr+OZ6ecgq89o/31KnyE7JKJcfZzJghZxkSWZqsLLlERf7WnX//LXiehwfQunVe687jBivv3g107SrXqJo7Fxg3zmQvwVwYbophk+EGkGMJ3ntP9vUmJxf8JFAcV9fCw8+jQahy5dJPk71/X3Y9PBpezp9//BRmP7+88KINMnXqyIF6DDFlj1oNHD+e17Jz4IBcA+pRwcHADz/I8TpE1uTaNf2wc+yY/Bv6qNq19Vt3mjaVg5WvXJFdW3fuyG6pZctsosWS4aYYNhtu8tNo5C/1jRt5l6Skwm8X9qZQFAcH2d31uFagS5cKBpjERNkkWxQvr4IBpm5d+Z9X6ZYlsmwPH8rFyLQtO9euyXE2o0ezW5Jsw8OHwKlTeV1Zhw8DZ88WPE87WPnWLfl3t0ULGf5t5EMgw00xykS4MUR6evHhR3t5dMxDaXh4FGx90d5mlwERUclpByvnH7+Tf7BylSpyhlRAgHI1GhnDTTEYbkopJ0d2dz2uJSg9HQgMLBhe6tSRrTM20DRKRGRxhJCtNYcPy1ae116TU79tiCHv32yzpZJxdJSfAGzoUwARkc1QqfImVRC4iQoRERHZFIYbIiIisikMN0RERGRTGG6IiIjIpjDcEBERkU1huCEiIiKbwnBDRERENsUiws28efNQs2ZNODs7IyQkBNHR0SW636pVq6BSqdCnTx/TFkhERERWQ/Fws3r1akRERGD69OmIjY1FUFAQunXrhps3bxZ7vytXruDdd99F+/btzVQpERERWQPFw81XX32F4cOHY/DgwWjYsCEWLFiA8uXLY8mSJUXeR61WY8CAAfjoo4/w1FNPFfv42dnZSEtL07sQERGR7VI03OTk5CAmJgZhYWG6Y3Z2dggLC0NUVFSR9/v444/h7e2NoUOHPvY5Zs2aBQ8PD90lgNsHEBER2TRFw83t27ehVqvh4+Ojd9zHxwdJSUmF3ufgwYNYvHgxFi1aVKLnmDx5MlJTU3WXxMTEJ66biIiILJdVbZyZnp6OgQMHYtGiRfDy8irRfZycnODk5GTiyoiIiMhSKBpuvLy8YG9vj+TkZL3jycnJ8PX1LXD+xYsXceXKFfTu3Vt3TKPRAADKlSuHs2fPolatWqYtmoiIiCyaouHG0dERLVq0QGRkpG46t0ajQWRkJMaMGVPg/Pr16+PkyZN6x6ZMmYL09HR8/fXXJRpPI4QAAA4sJiIisiLa923t+3hxFO+WioiIwKBBg9CyZUu0bt0ac+fORWZmJgYPHgwACA8Ph7+/P2bNmgVnZ2c0btxY7/4VK1YEgALHi5Keng4AHFhMRERkhdLT0+Hh4VHsOYqHm379+uHWrVuYNm0akpKS0KxZM2zbtk03yDghIQF2dsYb91y1alUkJibCzc0NKpXKaI8LyFQZEBCAxMREuLu7G/WxrUFZf/0Avwdl/fUD/B7w9Zft1w+Y7nsghEB6ejqqVq362HNVoiTtO1QiaWlp8PDwQGpqapn8pS7rrx/g96Csv36A3wO+/rL9+gHL+B4ovogfERERkTEx3BAREZFNYbgxIicnJ0yfPr3MrqtT1l8/wO9BWX/9AL8HfP1l+/UDlvE94JgbIiIisilsuSEiIiKbwnBDRERENoXhhoiIiGwKww0RERHZFIYbI5k3bx5q1qwJZ2dnhISEIDo6WumSzGbWrFlo1aoV3Nzc4O3tjT59+uDs2bNKl6WY//u//4NKpcL48eOVLsWsrl27htdffx2enp5wcXFBkyZNcOzYMaXLMgu1Wo2pU6ciMDAQLi4uqFWrFj755JMS7YFjrfbv34/evXujatWqUKlU+P333/W+LoTAtGnT4OfnBxcXF4SFheH8+fPKFGsCxb3+hw8f4v3330eTJk1QoUIFVK1aFeHh4bh+/bpyBZvA434H8hs5ciRUKhXmzp1rltoYboxg9erViIiIwPTp0xEbG4ugoCB069YNN2/eVLo0s9i3bx9Gjx6Nw4cPY+fOnXj48CG6du2KzMxMpUszu6NHj+KHH35A06ZNlS7FrO7du4e2bdvCwcEBW7duxenTpzF79mxUqlRJ6dLM4rPPPsP8+fPx3XffIT4+Hp999hk+//xzfPvtt0qXZjKZmZkICgrCvHnzCv36559/jm+++QYLFizAkSNHUKFCBXTr1g0PHjwwc6WmUdzrz8rKQmxsLKZOnYrY2Fhs2LABZ8+exfPPP69ApabzuN8Brd9++w2HDx8u0bYJRiPoibVu3VqMHj1ad1utVouqVauKWbNmKViVcm7evCkAiH379ildilmlp6eLOnXqiJ07d4qOHTuKcePGKV2S2bz//vuiXbt2SpehmF69eokhQ4boHXvxxRfFgAEDFKrIvACI3377TXdbo9EIX19f8cUXX+iOpaSkCCcnJ/Hrr78qUKFpPfr6CxMdHS0AiKtXr5qnKDMr6nvw77//Cn9/f3Hq1ClRo0YNMWfOHLPUw5abJ5STk4OYmBiEhYXpjtnZ2SEsLAxRUVEKVqac1NRUAEDlypUVrsS8Ro8ejV69eun9LpQVmzZtQsuWLfHKK6/A29sbwcHBWLRokdJlmU2bNm0QGRmJc+fOAQBOnDiBgwcPokePHgpXpozLly8jKSlJ7/+Ch4cHQkJCyvTfRZVKhYoVKypditloNBoMHDgQEydORKNGjcz63IrvCm7tbt++DbVardvFXMvHxwdnzpxRqCrlaDQajB8/Hm3btkXjxo2VLsdsVq1ahdjYWBw9elTpUhRx6dIlzJ8/HxEREfjggw9w9OhRvP3223B0dMSgQYOULs/kJk2ahLS0NNSvXx/29vZQq9X49NNPMWDAAKVLU0RSUhIAFPp3Ufu1suTBgwd4//338eqrr5apzTQ/++wzlCtXDm+//bbZn5vhhoxq9OjROHXqFA4ePKh0KWaTmJiIcePGYefOnXB2dla6HEVoNBq0bNkSM2fOBAAEBwfj1KlTWLBgQZkIN2vWrMGKFSuwcuVKNGrUCHFxcRg/fjyqVq1aJl4/Fe3hw4fo27cvhBCYP3++0uWYTUxMDL7++mvExsZCpVKZ/fnZLfWEvLy8YG9vj+TkZL3jycnJ8PX1VagqZYwZMwZ//vkn9uzZg2rVqildjtnExMTg5s2baN68OcqVK4dy5cph3759+Oabb1CuXDmo1WqlSzQ5Pz8/NGzYUO9YgwYNkJCQoFBF5jVx4kRMmjQJ/fv3R5MmTTBw4EC88847mDVrltKlKUL7t6+s/13UBpurV69i586dZarV5sCBA7h58yaqV6+u+7t49epVTJgwATVr1jT58zPcPCFHR0e0aNECkZGRumMajQaRkZEIDQ1VsDLzEUJgzJgx+O2337B7924EBgYqXZJZde7cGSdPnkRcXJzu0rJlSwwYMABxcXGwt7dXukSTa9u2bYHp/+fOnUONGjUUqsi8srKyYGen/+fU3t4eGo1GoYqUFRgYCF9fX72/i2lpaThy5EiZ+buoDTbnz5/Hrl274OnpqXRJZjVw4ED8/fffen8Xq1atiokTJ2L79u0mf352SxlBREQEBg0ahJYtW6J169aYO3cuMjMzMXjwYKVLM4vRo0dj5cqV2LhxI9zc3HR96h4eHnBxcVG4OtNzc3MrML6oQoUK8PT0LDPjjt555x20adMGM2fORN++fREdHY2FCxdi4cKFSpdmFr1798ann36K6tWro1GjRjh+/Di++uorDBkyROnSTCYjIwMXLlzQ3b58+TLi4uJQuXJlVK9eHePHj8eMGTNQp04dBAYGYurUqahatSr69OmjXNFGVNzr9/Pzw8svv4zY2Fj8+eefUKvVur+LlStXhqOjo1JlG9XjfgceDXQODg7w9fVFvXr1TF+cWeZklQHffvutqF69unB0dBStW7cWhw8fVrokswFQ6GXp0qVKl6aYsjYVXAgh/vjjD9G4cWPh5OQk6tevLxYuXKh0SWaTlpYmxo0bJ6pXry6cnZ3FU089JT788EORnZ2tdGkms2fPnkL/3w8aNEgIIaeDT506Vfj4+AgnJyfRuXNncfbsWWWLNqLiXv/ly5eL/Lu4Z88epUs3msf9DjzKnFPBVULY8BKaREREVOZwzA0RERHZFIYbIiIisikMN0RERGRTGG6IiIjIpjDcEBERkU1huCEiIiKbwnBDRERENoXhhoiIiGwKww0RlUkqlQq///670mUQkQkw3BCR2b3xxhtQqVQFLt27d1e6NCKyAdw4k4gU0b17dyxdulTvmJOTk0LVEJEtYcsNESnCyckJvr6+epdKlSoBkF1G8+fPR48ePeDi4oKnnnoK69at07v/yZMn8eyzz8LFxQWenp4YMWIEMjIy9M5ZsmQJGjVqBCcnJ/j5+WHMmDF6X799+zZeeOEFlC9fHnXq1MGmTZt0X7t37x4GDBiAKlWqwMXFBXXq1CkQxojIMjHcEJFFmjp1Kl566SWcOHECAwYMQP/+/REfHw8AyMzMRLdu3VCpUiUcPXoUa9euxa5du/TCy/z58zF69GiMGDECJ0+exKZNm1C7dm295/joo4/Qt29f/P333+jZsycGDBiAu3fv6p7/9OnT2Lp1K+Lj4zF//nx4eXmZ7xtARKVnlr3HiYjyGTRokLC3txcVKlTQu3z66adCCCEAiJEjR+rdJyQkRIwaNUoIIcTChQtFpUqVREZGhu7rmzdvFnZ2diIpKUkIIUTVqlXFhx9+WGQNAMSUKVN0tzMyMgQAsXXrViGEEL179xaDBw82zgsmIrPimBsiUsQzzzyD+fPn6x2rXLmy7npoaKje10JDQxEXFwcAiI+PR1BQECpUqKD7etu2baHRaHD27FmoVCpcv34dnTt3LraGpk2b6q5XqFAB7u7uuHnzJgBg1KhReOmllxAbG4uuXbuiT58+aNOmTaleKxGZF8MNESmiQoUKBbqJjMXFxaVE5zk4OOjdVqlU0Gg0AIAePXrg6tWr2LJlC3bu3InOnTtj9OjR+PLLL41eLxEZF8fcEJFFOnz4cIHbDRo0AAA0aNAAJ06cQGZmpu7rhw4dgp2dHerVqwc3NzfUrFkTkZGRT1RDlSpVMGjQIPzyyy+YO3cuFi5c+ESPR0TmwZYbIlJEdnY2kpKS9I6VK1dON2h37dq1aNmyJdq1a4cVK1YgOjoaixcvBgAMGDAA06dPx6BBg/Df//4Xt27dwtixYzFw4ED4+PgAAP773/9i5MiR8Pb2Ro8ePZCeno5Dhw5h7NixJapv2rRpaNGiBRo1aoTs7Gz8+eefunBFRJaN4YaIFLFt2zb4+fnpHatXrx7OnDkDQM5kWrVqFd566y34+fnh119/RcOGDQEA5cuXx/bt2zFu3Di0atUK5cuXx0svvYSvvvpK91iDBg3CgwcPMGfOHLz77rvw8vLCyy+/XOL6HB0dMXnyZFy5cgUuLi5o3749Vq1aZYRXTkSmphJCCKWLICLKT6VS4bfffkOfPn2ULoWIrBDH3BAREZFNYbghIiIim8IxN0RkcdhbTkRPgi03REREZFMYboiIiMimMNwQERGRTWG4ISIiIpvCcENEREQ2heGGiIiIbArDDREREdkUhhsiIiKyKf8PeC62aItS5kgAAAAASUVORK5CYII=","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# The input shape to use in the first hidden layer\n","input_shape = (n_cols,)\n","\n","# Create the new model: model_2\n","model_2 = Sequential()\n","\n","# Add the first, second, and third hidden layers\n","model_2.add(Dense(10,activation='relu',input_shape=input_shape))\n","model_2.add(Dense(10,activation='relu'))\n","model_2.add(Dense(10,activation='relu'))\n","\n","# Add the output layer\n","model_2.add(Dense(2,activation='softmax'))\n","\n","# Compile model_2\n","model_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","# Fit model 1\n","model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.4, verbose=False)\n","\n","# Fit model 2\n","model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.4, verbose=False)\n","\n","# Create the plot\n","plt.plot(model_1_training.history['val_loss'], 'r',label='model_1')\n","plt.plot(model_2_training.history['val_loss'], 'b',label='model_2')\n","plt.xlabel('Epochs')\n","plt.legend(loc=\"upper right\")\n","plt.ylabel('Validation score')\n","plt.show()"]},{"cell_type":"markdown","id":"4b7d7ec1-3eac-4c09-a899-edb69280f883","metadata":{},"source":["**Thinking about model capacity**  \n","\"Model capacity\" should be one of the key considerations you think about when deciding what models to try.\"Model capacity\" or \"network capacity\" is closely related to the terms overfitting and underfitting.  \n","  \n","Our validation score, is the ultimate measure of a model's predictive quality.\n","  \n","  Model capacity is a model's ability to capture predictive patterns in your data.  \n","*  If you had a network, and you increased the number of nodes or neurons in a hidden layer, that would increase model capacity. And if you add layers, that increases capacity.  \n","  \n","**Workflow for optimizing model capacity**\n","* Start with a small network  \n","* Gradually increase capacity\n","* Keep increasing capacity until validation score is no longer improving"]},{"cell_type":"markdown","id":"879a9fbc-bd96-4614-85dc-59e6f89f4431","metadata":{},"source":["## Stepping up images  \n","Using the famous MNIST dataset. \n","  \n","Building your own digit recognition model\n","\n","We've already done the basic manipulation of the MNIST dataset shown in the video, so you have X and y loaded and ready to model with. Sequential and Dense from tensorflow.keras are also pre-imported.\n","\n","To add an extra challenge, we've loaded only 2500 images, rather than 60000 which you will see in some published results. Deep learning models perform better with more data, however, they also take longer to train, especially when they start becoming more complex.\n","\n","If you have a computer with a CUDA compatible GPU, you can take advantage of it to improve computation time. If you don't have a GPU, no problem! You can set up a deep learning environment in the cloud that can run your models on a GPU. Here is a blog post by Dan that explains how to do this - check it out after completing this exercise! It is a great next step as you continue your deep learning journey."]},{"cell_type":"code","execution_count":31,"id":"882a97c1-5949-4d94-a922-1e09f5671a21","metadata":{"executionTime":22138,"lastSuccessfullyExecutedCode":"from keras.datasets import mnist\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\n\n(train_images, train_labels), (test_images, test_labels)=mnist.load_data()\nprint(\"The shape of the train data is: \", train_images.shape)\nprint(\"The shape of the test data is: \",test_images.shape)\n\n# Preparing the image data. preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [0, 1] interval. Previously, our training images, for instance, were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1. \n\ntrain_images=train_images.reshape((60000, 28*28))\ntrain_images=train_images.astype('float32')/255\n\ntest_images=test_images.reshape((10000,28*28))\ntest_images=test_images.astype('float32')/255\n\ntrain_labels=to_categorical(train_labels)\ntest_labels=to_categorical(test_labels)\n\n# Preparing the model\nmodel=Sequential()\nmodel.add(Dense(512,activation='relu',input_shape=(28*28,)))\nmodel.add(Dense(10,activation='softmax'))\nmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.fit(train_images,train_labels,epochs=5,batch_size=128)\n"},"outputs":[{"name":"stdout","output_type":"stream","text":["The shape of the train data is:  (60000, 28, 28)\n","The shape of the test data is:  (10000, 28, 28)\n","Epoch 1/5\n","469/469 [==============================] - 5s 9ms/step - loss: 0.2606 - accuracy: 0.9241\n","Epoch 2/5\n","469/469 [==============================] - 4s 9ms/step - loss: 0.1034 - accuracy: 0.9697\n","Epoch 3/5\n","469/469 [==============================] - 4s 9ms/step - loss: 0.0688 - accuracy: 0.9795\n","Epoch 4/5\n","469/469 [==============================] - 4s 9ms/step - loss: 0.0500 - accuracy: 0.9851\n","Epoch 5/5\n","469/469 [==============================] - 4s 9ms/step - loss: 0.0382 - accuracy: 0.9886\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fbe2177e940>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["from keras.datasets import mnist\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","\n","(train_images, train_labels), (test_images, test_labels)=mnist.load_data()\n","print(\"The shape of the train data is: \", train_images.shape)\n","print(\"The shape of the test data is: \",test_images.shape)\n","\n","# Preparing the image data. preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [0, 1] interval. Previously, our training images, for instance, were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1. \n","\n","train_images=train_images.reshape((60000, 28*28))\n","train_images=train_images.astype('float32')/255\n","\n","test_images=test_images.reshape((10000,28*28))\n","test_images=test_images.astype('float32')/255\n","\n","train_labels=to_categorical(train_labels)\n","test_labels=to_categorical(test_labels)\n","\n","# Preparing the model\n","model=Sequential()\n","model.add(Dense(512,activation='relu',input_shape=(28*28,)))\n","model.add(Dense(10,activation='softmax'))\n","model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n","model.fit(train_images,train_labels,epochs=5,batch_size=128)\n"]},{"cell_type":"code","execution_count":32,"id":"5d6689db-b77f-4525-99cc-f5269d912441","metadata":{"executionTime":648,"lastSuccessfullyExecutedCode":"test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('test_acc: ',test_acc)"},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 1s 1ms/step - loss: 0.0619 - accuracy: 0.9809\n","test_acc:  0.98089998960495\n"]}],"source":["test_loss, test_acc = model.evaluate(test_images, test_labels)\n","print('test_acc: ',test_acc)"]}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"vscode":{"interpreter":{"hash":"8f5527555e53236f4559d0028b82e3fad8a11d14d9b8a9a52c5ce72ecf5660e3"}}},"nbformat":4,"nbformat_minor":5}
